{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n%pip install -U transformers datasets accelerate peft trl bitsandbytes wandb","metadata":{"execution":{"iopub.status.busy":"2024-09-20T18:13:41.231114Z","iopub.execute_input":"2024-09-20T18:13:41.231916Z","iopub.status.idle":"2024-09-20T18:14:37.950014Z","shell.execute_reply.started":"2024-09-20T18:13:41.231876Z","shell.execute_reply":"2024-09-20T18:14:37.948760Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    TrainingArguments,\n    pipeline,\n    logging,\n)\nfrom peft import (\n    LoraConfig,\n    PeftModel,\n    prepare_model_for_kbit_training,\n    get_peft_model,\n)\nimport os, torch, wandb\nfrom datasets import load_dataset\nfrom trl import SFTTrainer, setup_chat_format\nfrom accelerate import Accelerator","metadata":{"id":"VLzgZ14X_rMs","execution":{"iopub.status.busy":"2024-09-20T18:14:37.952629Z","iopub.execute_input":"2024-09-20T18:14:37.952991Z","iopub.status.idle":"2024-09-20T18:15:01.359029Z","shell.execute_reply.started":"2024-09-20T18:14:37.952950Z","shell.execute_reply":"2024-09-20T18:15:01.357936Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\n\nhf_token = user_secrets.get_secret(\"HF_TOKEN\")\nlogin(token = hf_token)","metadata":{"execution":{"iopub.status.busy":"2024-09-20T18:15:01.360337Z","iopub.execute_input":"2024-09-20T18:15:01.361733Z","iopub.status.idle":"2024-09-20T18:15:01.603777Z","shell.execute_reply.started":"2024-09-20T18:15:01.361693Z","shell.execute_reply":"2024-09-20T18:15:01.602413Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"wb_token = user_secrets.get_secret(\"wandb\")\n\nwandb.login(key=wb_token)\nrun = wandb.init(\n    project='Fine-tune CodeLlama 2 7B for JUnit Test case Generation', \n    job_type=\"training\", \n    anonymous=\"allow\"\n)","metadata":{"id":"na9CAoHC5gM9","execution":{"iopub.status.busy":"2024-09-20T18:15:01.606441Z","iopub.execute_input":"2024-09-20T18:15:01.606848Z","iopub.status.idle":"2024-09-20T18:15:04.857550Z","shell.execute_reply.started":"2024-09-20T18:15:01.606802Z","shell.execute_reply":"2024-09-20T18:15:04.856493Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwalim\u001b[0m (\u001b[33miHateNLP\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240920_181502-4qb0qa5x</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/iHateNLP/Fine-tune%20CodeLlama%202%207B%20for%20JUnit%20Test%20case%20Generation/runs/4qb0qa5x' target=\"_blank\">fluent-glade-5</a></strong> to <a href='https://wandb.ai/iHateNLP/Fine-tune%20CodeLlama%202%207B%20for%20JUnit%20Test%20case%20Generation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/iHateNLP/Fine-tune%20CodeLlama%202%207B%20for%20JUnit%20Test%20case%20Generation' target=\"_blank\">https://wandb.ai/iHateNLP/Fine-tune%20CodeLlama%202%207B%20for%20JUnit%20Test%20case%20Generation</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/iHateNLP/Fine-tune%20CodeLlama%202%207B%20for%20JUnit%20Test%20case%20Generation/runs/4qb0qa5x' target=\"_blank\">https://wandb.ai/iHateNLP/Fine-tune%20CodeLlama%202%207B%20for%20JUnit%20Test%20case%20Generation/runs/4qb0qa5x</a>"},"metadata":{}}]},{"cell_type":"code","source":"base_model = \"codellama/CodeLlama-7b-hf\"\ndataset_name = \"CodexAI/dataset\"\nnew_model = \"CODEX-CodeLlama-7b-hf\"","metadata":{"execution":{"iopub.status.busy":"2024-09-20T18:15:04.859310Z","iopub.execute_input":"2024-09-20T18:15:04.859669Z","iopub.status.idle":"2024-09-20T18:15:05.135930Z","shell.execute_reply.started":"2024-09-20T18:15:04.859631Z","shell.execute_reply":"2024-09-20T18:15:05.135039Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"acc = Accelerator()","metadata":{"execution":{"iopub.status.busy":"2024-09-20T18:15:05.137471Z","iopub.execute_input":"2024-09-20T18:15:05.138036Z","iopub.status.idle":"2024-09-20T18:15:05.405486Z","shell.execute_reply.started":"2024-09-20T18:15:05.137984Z","shell.execute_reply":"2024-09-20T18:15:05.404118Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Set torch dtype and attention implementation\nif torch.cuda.get_device_capability()[0] >= 8:\n    !pip install -qqq flash-attn\n    torch_dtype = torch.bfloat16\n    attn_implementation = \"flash_attention_2\"\nelse:\n    torch_dtype = torch.float16\n    attn_implementation = \"eager\"","metadata":{"execution":{"iopub.status.busy":"2024-09-20T18:15:05.410872Z","iopub.execute_input":"2024-09-20T18:15:05.411506Z","iopub.status.idle":"2024-09-20T18:15:05.721821Z","shell.execute_reply.started":"2024-09-20T18:15:05.411465Z","shell.execute_reply":"2024-09-20T18:15:05.720772Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"if torch.cuda.is_available():\n    print(f\"CUDA device: {torch.cuda.get_device_name()}\")\n    device={\"\":0}\n    torch_type=torch.bfloat16\nelse:\n    device=\"cpu\"\n    torch_type=torch.bfloat16\n    print(\"I am begging for mercy already!\")","metadata":{"execution":{"iopub.status.busy":"2024-09-20T18:15:05.723287Z","iopub.execute_input":"2024-09-20T18:15:05.723633Z","iopub.status.idle":"2024-09-20T18:15:05.988873Z","shell.execute_reply.started":"2024-09-20T18:15:05.723596Z","shell.execute_reply":"2024-09-20T18:15:05.987685Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"CUDA device: Tesla P100-PCIE-16GB\n","output_type":"stream"}]},{"cell_type":"code","source":"# QLoRA config\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch_type,\n    bnb_4bit_use_double_quant=True,\n)\n\n# Load model\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model,\n    quantization_config=bnb_config,\n    device_map=device,\n    attn_implementation=attn_implementation,\n)\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)","metadata":{"id":"StJKGiDDHzdk","outputId":"871214ba-6c30-4ecf-ac68-550f296b7ef6","execution":{"iopub.status.busy":"2024-09-20T18:15:05.990366Z","iopub.execute_input":"2024-09-20T18:15:05.990838Z","iopub.status.idle":"2024-09-20T18:17:05.855598Z","shell.execute_reply.started":"2024-09-20T18:15:05.990791Z","shell.execute_reply":"2024-09-20T18:17:05.854538Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/637 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f31a8fea85a457ba21653debb570691"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04b63ec03ec742d6851f434ca7a70c1f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d00630085053403d881d8bf7e0a8745a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4006c3cb57d04100ba33d9520b2faf22"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4cc6cd9cb6084465aebeb49d7777525c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48ed2970f1874f3b9e58dea7c81b5a46"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1c55ec1af664383a236642d00c38262"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/749 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e8a45c10d5b4d36a0799411e1f4cc46"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f1f598aea754a6eac6e0243c03aa6b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cbc691aba59f43e0bc6fce3a4540197c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/411 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29123b60802f48579d281d50d3f854d7"}},"metadata":{}}]},{"cell_type":"code","source":"model","metadata":{"execution":{"iopub.status.busy":"2024-09-20T18:17:05.859577Z","iopub.execute_input":"2024-09-20T18:17:05.859932Z","iopub.status.idle":"2024-09-20T18:17:07.013007Z","shell.execute_reply.started":"2024-09-20T18:17:05.859893Z","shell.execute_reply":"2024-09-20T18:17:07.012116Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(32016, 4096)\n    (layers): ModuleList(\n      (0-31): 32 x LlamaDecoderLayer(\n        (self_attn): LlamaAttention(\n          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n      )\n    )\n    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n    (rotary_emb): LlamaRotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=4096, out_features=32016, bias=False)\n)"},"metadata":{}}]},{"cell_type":"code","source":"# LoRA config\npeft_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n)\npeft_config","metadata":{"execution":{"iopub.status.busy":"2024-09-20T18:17:07.014240Z","iopub.execute_input":"2024-09-20T18:17:07.014593Z","iopub.status.idle":"2024-09-20T18:17:07.281710Z","shell.execute_reply.started":"2024-09-20T18:17:07.014555Z","shell.execute_reply":"2024-09-20T18:17:07.280522Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='CAUSAL_LM', inference_mode=False, r=16, target_modules={'o_proj', 'up_proj', 'v_proj', 'q_proj', 'down_proj', 'gate_proj', 'k_proj'}, lora_alpha=32, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False))"},"metadata":{}}]},{"cell_type":"code","source":"model, tokenizer = setup_chat_format(model, tokenizer)\nmodel = get_peft_model(model, peft_config)\nmodel","metadata":{"execution":{"iopub.status.busy":"2024-09-20T18:17:07.283615Z","iopub.execute_input":"2024-09-20T18:17:07.284099Z","iopub.status.idle":"2024-09-20T18:17:08.638014Z","shell.execute_reply.started":"2024-09-20T18:17:07.284045Z","shell.execute_reply":"2024-09-20T18:17:08.636462Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): LlamaForCausalLM(\n      (model): LlamaModel(\n        (embed_tokens): Embedding(32018, 4096)\n        (layers): ModuleList(\n          (0-31): 32 x LlamaDecoderLayer(\n            (self_attn): LlamaAttention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (rotary_emb): LlamaRotaryEmbedding()\n            )\n            (mlp): LlamaMLP(\n              (gate_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=11008, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=11008, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (up_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=11008, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=11008, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (down_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=11008, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=11008, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (act_fn): SiLU()\n            )\n            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n          )\n        )\n        (norm): LlamaRMSNorm((4096,), eps=1e-05)\n        (rotary_emb): LlamaRotaryEmbedding()\n      )\n      (lm_head): Linear(in_features=4096, out_features=32018, bias=False)\n    )\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"model.print_trainable_parameters()","metadata":{"execution":{"iopub.status.busy":"2024-09-20T18:17:08.639600Z","iopub.execute_input":"2024-09-20T18:17:08.639984Z","iopub.status.idle":"2024-09-20T18:17:08.941005Z","shell.execute_reply.started":"2024-09-20T18:17:08.639945Z","shell.execute_reply":"2024-09-20T18:17:08.939903Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"trainable params: 39,976,960 || all params: 6,778,540,032 || trainable%: 0.5898\n","output_type":"stream"}]},{"cell_type":"code","source":"dataset = load_dataset(dataset_name)\ndataset","metadata":{"execution":{"iopub.status.busy":"2024-09-20T18:17:08.942707Z","iopub.execute_input":"2024-09-20T18:17:08.943510Z","iopub.status.idle":"2024-09-20T18:17:11.855820Z","shell.execute_reply.started":"2024-09-20T18:17:08.943457Z","shell.execute_reply":"2024-09-20T18:17:11.854844Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/426 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52c05efce0394abd93c5261c91409639"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/35.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5d0065cbb724df49413cd9f2c8c7e36"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/1.74M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5066b6b3db4249a09e7636b3e494bdde"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/100000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"508325320a1d4a998b5744c0e212f035"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/4687 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8263235d0e064a01af3215d6d09eb293"}},"metadata":{}},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['focal_method', 'test_case'],\n        num_rows: 100000\n    })\n    test: Dataset({\n        features: ['focal_method', 'test_case'],\n        num_rows: 4687\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"train=dataset['train']\ntrain=train.shuffle(True).select(range(100))\ntrain","metadata":{"execution":{"iopub.status.busy":"2024-09-20T18:17:11.857272Z","iopub.execute_input":"2024-09-20T18:17:11.858444Z","iopub.status.idle":"2024-09-20T18:17:12.165459Z","shell.execute_reply.started":"2024-09-20T18:17:11.858364Z","shell.execute_reply":"2024-09-20T18:17:12.164437Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['focal_method', 'test_case'],\n    num_rows: 100\n})"},"metadata":{}}]},{"cell_type":"code","source":"test=dataset['test']\ntest=test.shuffle(True).select(range(10))\ntest","metadata":{"execution":{"iopub.status.busy":"2024-09-20T18:17:12.166649Z","iopub.execute_input":"2024-09-20T18:17:12.166961Z","iopub.status.idle":"2024-09-20T18:17:12.445561Z","shell.execute_reply.started":"2024-09-20T18:17:12.166927Z","shell.execute_reply":"2024-09-20T18:17:12.444457Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['focal_method', 'test_case'],\n    num_rows: 10\n})"},"metadata":{}}]},{"cell_type":"code","source":"def format_chat_template(row):\n    row_json = [\n        {\"role\": \"system\", \"content\": \"You are a coding assistant for generating JUnit test cases.\"},\n        {\"role\": \"user\", \"content\": row[\"focal_method\"]},\n        {\"role\": \"assistant\", \"content\": row[\"test_case\"]}\n    ]\n    row[\"text\"] = tokenizer.apply_chat_template(row_json, tokenize=False)\n    return row\n","metadata":{"execution":{"iopub.status.busy":"2024-09-20T18:17:12.447070Z","iopub.execute_input":"2024-09-20T18:17:12.448233Z","iopub.status.idle":"2024-09-20T18:17:12.727736Z","shell.execute_reply.started":"2024-09-20T18:17:12.448176Z","shell.execute_reply":"2024-09-20T18:17:12.726766Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"train = train.map(\n    format_chat_template,\n    num_proc= 4,\n)\n\ntrain","metadata":{"id":"XzF2UjPvTBag","outputId":"3733e45f-605e-4564-88c7-368c9c5bf9cd","execution":{"iopub.status.busy":"2024-09-20T18:17:12.728931Z","iopub.execute_input":"2024-09-20T18:17:12.729238Z","iopub.status.idle":"2024-09-20T18:17:13.465936Z","shell.execute_reply.started":"2024-09-20T18:17:12.729203Z","shell.execute_reply":"2024-09-20T18:17:13.464785Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"feb677c767b84211849de2c647f8ad7e"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['focal_method', 'test_case', 'text'],\n    num_rows: 100\n})"},"metadata":{}}]},{"cell_type":"code","source":"test = test.map(\n    format_chat_template,\n    num_proc= 4,\n)\n\ntest","metadata":{"execution":{"iopub.status.busy":"2024-09-20T18:17:13.467869Z","iopub.execute_input":"2024-09-20T18:17:13.468350Z","iopub.status.idle":"2024-09-20T18:17:14.168726Z","shell.execute_reply.started":"2024-09-20T18:17:13.468308Z","shell.execute_reply":"2024-09-20T18:17:14.167606Z"},"trusted":true},"execution_count":19,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/10 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c82d68ab7b4e4cf1a8476b3ec5862a21"}},"metadata":{}},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['focal_method', 'test_case', 'text'],\n    num_rows: 10\n})"},"metadata":{}}]},{"cell_type":"code","source":"train['text'][3]","metadata":{"execution":{"iopub.status.busy":"2024-09-20T18:17:14.170465Z","iopub.execute_input":"2024-09-20T18:17:14.170945Z","iopub.status.idle":"2024-09-20T18:17:14.409020Z","shell.execute_reply.started":"2024-09-20T18:17:14.170871Z","shell.execute_reply":"2024-09-20T18:17:14.407585Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"'<|im_start|>system\\nYou are a coding assistant for generating JUnit test cases.<|im_end|>\\n<|im_start|>user\\npublic OutputStream createRawOutputStream() throws IOException\\n    {\\n        checkClosed();\\n        if (isWriting)\\n        {\\n            throw new IllegalStateException(\"Cannot have more than one open stream writer.\");\\n        }\\n        if (randomAccess != null)\\n            randomAccess.clear();\\n        else\\n            randomAccess = getStreamCache().createBuffer();\\n        OutputStream out = new RandomAccessOutputStream(randomAccess);\\n        isWriting = true;\\n        return new FilterOutputStream(out)\\n        {\\n            @Override\\n            public void write(byte[] b, int off, int len) throws IOException\\n            {\\n                this.out.write(b, off, len);\\n            }\\n            \\n            @Override\\n            public void close() throws IOException\\n            {\\n                super.close();\\n                setInt(COSName.LENGTH, (int)randomAccess.length());\\n                isWriting = false;\\n            }\\n        };\\n    }<|im_end|>\\n<|im_start|>assistant\\n@Test\\n    void testCompressedStream1Decode() throws IOException\\n    {\\n        byte[] testString = \"This is a test string to be used as input for TestCOSStream\".getBytes(StandardCharsets.US_ASCII);\\n        byte[] testStringEncoded = encodeData(testString, COSName.FLATE_DECODE);\\n        COSStream stream = new COSStream();\\n        \\n        try (OutputStream output = stream.createRawOutputStream())\\n        {\\n            output.write(testStringEncoded);\\n        }\\n\\n        stream.setItem(COSName.FILTER, COSName.FLATE_DECODE);\\n        validateDecoded(stream, testString);\\n    }<|im_end|>\\n'"},"metadata":{}}]},{"cell_type":"code","source":"#Hyperparamter\ntraining_arguments = TrainingArguments(\n    output_dir=\"./results\",\n    run_name =\"./loggings\",\n    overwrite_output_dir=True,\n    eval_strategy=\"steps\",\n    eval_steps=0.10,\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    gradient_accumulation_steps=4,\n    optim=\"paged_adamw_32bit\",\n    num_train_epochs=1,\n    warmup_steps=10,\n    logging_strategy=\"steps\",\n    logging_steps=50,\n    logging_first_step=True,\n    learning_rate=2e-4,\n    fp16=False,\n    bf16=True,\n    group_by_length=True,\n    report_to=\"wandb\",\n)","metadata":{"id":"peOnLAAhS0y1","execution":{"iopub.status.busy":"2024-09-20T18:17:14.410376Z","iopub.execute_input":"2024-09-20T18:17:14.410789Z","iopub.status.idle":"2024-09-20T18:17:14.723537Z","shell.execute_reply.started":"2024-09-20T18:17:14.410750Z","shell.execute_reply":"2024-09-20T18:17:14.722442Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# Setting sft parameters\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=train,\n    eval_dataset=test,\n    peft_config=peft_config,\n    max_seq_length= 512,\n    dataset_text_field=\"text\",\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing= False,\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-20T18:17:14.724884Z","iopub.execute_input":"2024-09-20T18:17:14.725305Z","iopub.status.idle":"2024-09-20T18:17:15.330293Z","shell.execute_reply.started":"2024-09-20T18:17:14.725250Z","shell.execute_reply":"2024-09-20T18:17:15.328933Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length, dataset_text_field. Will not be supported from version '1.0.0'.\n\nDeprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n  warnings.warn(message, FutureWarning)\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:321: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e54aa8c021e48f88309132d083e9a7b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67149c2950464ecbaf3c8e23ec249c6e"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:396: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer.train_dataset","metadata":{"execution":{"iopub.status.busy":"2024-09-20T18:17:15.331993Z","iopub.execute_input":"2024-09-20T18:17:15.332972Z","iopub.status.idle":"2024-09-20T18:17:15.635251Z","shell.execute_reply.started":"2024-09-20T18:17:15.332899Z","shell.execute_reply":"2024-09-20T18:17:15.634270Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['input_ids', 'attention_mask'],\n    num_rows: 100\n})"},"metadata":{}}]},{"cell_type":"code","source":"trainer.eval_dataset","metadata":{"execution":{"iopub.status.busy":"2024-09-20T18:17:15.636515Z","iopub.execute_input":"2024-09-20T18:17:15.636841Z","iopub.status.idle":"2024-09-20T18:17:15.946042Z","shell.execute_reply.started":"2024-09-20T18:17:15.636806Z","shell.execute_reply":"2024-09-20T18:17:15.944840Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['input_ids', 'attention_mask'],\n    num_rows: 10\n})"},"metadata":{}}]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-09-20T18:17:15.947406Z","iopub.execute_input":"2024-09-20T18:17:15.947759Z","iopub.status.idle":"2024-09-20T18:24:03.512301Z","shell.execute_reply.started":"2024-09-20T18:17:15.947721Z","shell.execute_reply":"2024-09-20T18:24:03.511429Z"},"trusted":true},"execution_count":25,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [25/25 06:28, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>3</td>\n      <td>1.255000</td>\n      <td>1.760233</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>1.255000</td>\n      <td>1.632642</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>1.255000</td>\n      <td>1.403932</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>1.255000</td>\n      <td>1.076716</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>1.255000</td>\n      <td>0.975671</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>1.255000</td>\n      <td>0.954687</td>\n    </tr>\n    <tr>\n      <td>21</td>\n      <td>1.255000</td>\n      <td>0.934519</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>1.255000</td>\n      <td>0.922700</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:232: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=25, training_loss=1.1696454191207886, metrics={'train_runtime': 405.675, 'train_samples_per_second': 0.247, 'train_steps_per_second': 0.062, 'total_flos': 1243434988929024.0, 'train_loss': 1.1696454191207886, 'epoch': 1.0})"},"metadata":{}}]},{"cell_type":"code","source":"trainer.model","metadata":{"execution":{"iopub.status.busy":"2024-09-20T18:24:03.513912Z","iopub.execute_input":"2024-09-20T18:24:03.514841Z","iopub.status.idle":"2024-09-20T18:24:03.824324Z","shell.execute_reply.started":"2024-09-20T18:24:03.514788Z","shell.execute_reply":"2024-09-20T18:24:03.823439Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): LlamaForCausalLM(\n      (model): LlamaModel(\n        (embed_tokens): Embedding(32018, 4096)\n        (layers): ModuleList(\n          (0-31): 32 x LlamaDecoderLayer(\n            (self_attn): LlamaAttention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (rotary_emb): LlamaRotaryEmbedding()\n            )\n            (mlp): LlamaMLP(\n              (gate_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=11008, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=11008, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (up_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=11008, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=11008, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (down_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=11008, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=11008, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (act_fn): SiLU()\n            )\n            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n          )\n        )\n        (norm): LlamaRMSNorm((4096,), eps=1e-05)\n        (rotary_emb): LlamaRotaryEmbedding()\n      )\n      (lm_head): Linear(in_features=4096, out_features=32018, bias=False)\n    )\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"trainer.model.save_pretrained(new_model)\nwandb.finish()\nmodel.config.use_cache = True","metadata":{"id":"nKgZBEGVS5a2","execution":{"iopub.status.busy":"2024-09-20T18:24:03.825663Z","iopub.execute_input":"2024-09-20T18:24:03.826015Z","iopub.status.idle":"2024-09-20T18:24:09.351445Z","shell.execute_reply.started":"2024-09-20T18:24:03.825977Z","shell.execute_reply":"2024-09-20T18:24:09.349964Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:232: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.324 MB of 0.324 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▇▅▂▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▆▆█▂▃▁▃▄</td></tr><tr><td>eval/samples_per_second</td><td>██▁█████</td></tr><tr><td>eval/steps_per_second</td><td>██▁█████</td></tr><tr><td>train/epoch</td><td>▁▂▂▃▄▅▆▇██</td></tr><tr><td>train/global_step</td><td>▁▂▂▃▄▅▆▇██</td></tr><tr><td>train/grad_norm</td><td>▁</td></tr><tr><td>train/learning_rate</td><td>▁</td></tr><tr><td>train/loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.9227</td></tr><tr><td>eval/runtime</td><td>13.8935</td></tr><tr><td>eval/samples_per_second</td><td>0.72</td></tr><tr><td>eval/steps_per_second</td><td>0.72</td></tr><tr><td>total_flos</td><td>1243434988929024.0</td></tr><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/global_step</td><td>25</td></tr><tr><td>train/grad_norm</td><td>0.29396</td></tr><tr><td>train/learning_rate</td><td>2e-05</td></tr><tr><td>train/loss</td><td>1.255</td></tr><tr><td>train_loss</td><td>1.16965</td></tr><tr><td>train_runtime</td><td>405.675</td></tr><tr><td>train_samples_per_second</td><td>0.247</td></tr><tr><td>train_steps_per_second</td><td>0.062</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">fluent-glade-5</strong> at: <a href='https://wandb.ai/iHateNLP/Fine-tune%20CodeLlama%202%207B%20for%20JUnit%20Test%20case%20Generation/runs/4qb0qa5x' target=\"_blank\">https://wandb.ai/iHateNLP/Fine-tune%20CodeLlama%202%207B%20for%20JUnit%20Test%20case%20Generation/runs/4qb0qa5x</a><br/> View project at: <a href='https://wandb.ai/iHateNLP/Fine-tune%20CodeLlama%202%207B%20for%20JUnit%20Test%20case%20Generation' target=\"_blank\">https://wandb.ai/iHateNLP/Fine-tune%20CodeLlama%202%207B%20for%20JUnit%20Test%20case%20Generation</a><br/>Synced 5 W&B file(s), 0 media file(s), 10 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240920_181502-4qb0qa5x/logs</code>"},"metadata":{}}]},{"cell_type":"code","source":"messages = [{\"role\": \"user\", \"content\": \"\"\"\npublic class SimpleCalculator {\n    // Method to add two numbers\n    public int add(int a, int b) {\n        return a + b;\n    }\n\n    // Method to subtract two numbers\n    public int subtract(int a, int b) {\n        return a - b;\n    }\n\n    // Method to multiply two numbers\n    public int multiply(int a, int b) {\n        return a * b;\n    }\n\n    // Method to divide two numbers\n    // Throws ArithmeticException if divisor is zero\n    public double divide(int a, int b) {\n        if (b == 0) {\n            throw new ArithmeticException(\"Cannot divide by zero\");\n        }\n        return (double) a / b;\n    }\n}\n\"\"\"}]\n\nprompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    \ninputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n\noutputs = model.generate(**inputs, max_new_tokens=512, num_return_sequences=1)\n\ntext = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\nprint(text.split(\"assistant\")[1])","metadata":{"id":"L7vHP41ITQPb","execution":{"iopub.status.busy":"2024-09-20T18:24:09.360838Z","iopub.execute_input":"2024-09-20T18:24:09.361397Z","iopub.status.idle":"2024-09-20T18:25:34.904285Z","shell.execute_reply.started":"2024-09-20T18:24:09.361311Z","shell.execute_reply":"2024-09-20T18:25:34.903184Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stderr","text":"Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","output_type":"stream"},{"name":"stdout","text":"\n@Test\n    public void testAdd() {\n        SimpleCalculator calculator = new SimpleCalculator();\n        assertEquals(4, calculator.add(2, 2));\n    }\n\n    @Test\n    public void testSubtract() {\n        SimpleCalculator calculator = new SimpleCalculator();\n        assertEquals(2, calculator.subtract(4, 2));\n    }\n\n    @Test\n    public void testMultiply() {\n        SimpleCalculator calculator = new SimpleCalculator();\n        assertEquals(4, calculator.multiply(2, 2));\n    }\n\n    @Test\n    public void testDivide() {\n        SimpleCalculator calculator = new SimpleCalculator();\n        assertEquals(2.0, calculator.divide(4, 2), 0.0001);\n    }\n\n    @Test(expected = ArithmeticException.class)\n    public void testDivideByZero() {\n        SimpleCalculator calculator = new SimpleCalculator();\n        calculator.divide(4, 0);\n    }\n\n    @Test\n    public void testDivideByZeroWithMessage() {\n        SimpleCalculator calculator = new SimpleCalculator();\n        try {\n            calculator.divide(4, 0);\n            fail(\"Expected ArithmeticException\");\n        } catch (ArithmeticException e) {\n            assertEquals(\"Cannot divide by zero\", e.getMessage());\n        }\n    }\n\n    @Test\n    public void testDivideByZeroWithMessageAndCause() {\n        SimpleCalculator calculator = new SimpleCalculator();\n        try {\n            calculator.divide(4, 0);\n            fail(\"Expected ArithmeticException\");\n        } catch (ArithmeticException e) {\n            assertEquals(\"Cannot divide by zero\", e.getMessage());\n            assertNotNull(e.getCause());\n        }\n    }\n\n    @Test\n    public void testDivideByZeroWithMessageAndCauseAndSuppressed() {\n        SimpleCalculator calculator = new SimpleCalculator();\n        try {\n            calculator.divide(4, 0);\n            fail(\"Expected ArithmeticException\");\n","output_type":"stream"}]},{"cell_type":"code","source":"from huggingface_hub import HfApi, create_repo","metadata":{"execution":{"iopub.status.busy":"2024-09-20T18:25:34.905649Z","iopub.execute_input":"2024-09-20T18:25:34.905956Z","iopub.status.idle":"2024-09-20T18:25:34.910698Z","shell.execute_reply.started":"2024-09-20T18:25:34.905922Z","shell.execute_reply":"2024-09-20T18:25:34.909636Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"repo_name = new_model\norganization_name = \"CodexAI\"\nrepo_url = f\"{organization_name}/{repo_name}\"","metadata":{"execution":{"iopub.status.busy":"2024-09-20T18:25:34.912238Z","iopub.execute_input":"2024-09-20T18:25:34.913040Z","iopub.status.idle":"2024-09-20T18:25:34.923454Z","shell.execute_reply.started":"2024-09-20T18:25:34.912989Z","shell.execute_reply":"2024-09-20T18:25:34.922441Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"create_repo(repo_url, repo_type=\"model\", private=True,exist_ok=True)","metadata":{"execution":{"iopub.status.busy":"2024-09-20T18:25:34.924658Z","iopub.execute_input":"2024-09-20T18:25:34.925002Z","iopub.status.idle":"2024-09-20T18:25:35.507661Z","shell.execute_reply.started":"2024-09-20T18:25:34.924968Z","shell.execute_reply":"2024-09-20T18:25:35.506432Z"},"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"RepoUrl('https://huggingface.co/CodexAI/CODEX-CodeLlama-7b-hf', endpoint='https://huggingface.co', repo_type='model', repo_id='CodexAI/CODEX-CodeLlama-7b-hf')"},"metadata":{}}]},{"cell_type":"code","source":"api = HfApi()\napi.upload_folder(folder_path=new_model,repo_id=repo_url)","metadata":{"execution":{"iopub.status.busy":"2024-09-20T18:25:35.509360Z","iopub.execute_input":"2024-09-20T18:25:35.510209Z","iopub.status.idle":"2024-09-20T18:25:55.410741Z","shell.execute_reply.started":"2024-09-20T18:25:35.510158Z","shell.execute_reply":"2024-09-20T18:25:55.409597Z"},"trusted":true},"execution_count":32,"outputs":[{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/685M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2afb8ae576924d7cae34e5b02626e5d0"}},"metadata":{}},{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/CodexAI/CODEX-CodeLlama-7b-hf/commit/5a67c77f554abb4485f53da21e77ba75902d69f8', commit_message='Upload folder using huggingface_hub', commit_description='', oid='5a67c77f554abb4485f53da21e77ba75902d69f8', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}]},{"cell_type":"code","source":"print('END')","metadata":{"execution":{"iopub.status.busy":"2024-09-20T18:25:55.412193Z","iopub.execute_input":"2024-09-20T18:25:55.413062Z","iopub.status.idle":"2024-09-20T18:25:55.418810Z","shell.execute_reply.started":"2024-09-20T18:25:55.413008Z","shell.execute_reply":"2024-09-20T18:25:55.417461Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"END\n","output_type":"stream"}]}]}