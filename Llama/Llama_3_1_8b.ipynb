{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n%pip install -U transformers datasets accelerate peft trl bitsandbytes wandb","metadata":{"execution":{"iopub.status.busy":"2024-09-19T04:16:01.908251Z","iopub.execute_input":"2024-09-19T04:16:01.909113Z","iopub.status.idle":"2024-09-19T04:16:15.922824Z","shell.execute_reply.started":"2024-09-19T04:16:01.909067Z","shell.execute_reply":"2024-09-19T04:16:15.921692Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    TrainingArguments,\n    pipeline,\n    logging,\n)\nfrom peft import (\n    LoraConfig,\n    PeftModel,\n    prepare_model_for_kbit_training,\n    get_peft_model,\n)\nimport os, torch, wandb\nfrom datasets import load_dataset\nfrom trl import SFTTrainer, setup_chat_format\n","metadata":{"id":"VLzgZ14X_rMs","execution":{"iopub.status.busy":"2024-09-19T04:16:15.924867Z","iopub.execute_input":"2024-09-19T04:16:15.925203Z","iopub.status.idle":"2024-09-19T04:16:24.222244Z","shell.execute_reply.started":"2024-09-19T04:16:15.925168Z","shell.execute_reply":"2024-09-19T04:16:24.221214Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\n\nhf_token = user_secrets.get_secret(\"HF_TOKEN\")\nlogin(token = hf_token)","metadata":{"execution":{"iopub.status.busy":"2024-09-19T04:16:24.223450Z","iopub.execute_input":"2024-09-19T04:16:24.224056Z","iopub.status.idle":"2024-09-19T04:16:24.582878Z","shell.execute_reply.started":"2024-09-19T04:16:24.224021Z","shell.execute_reply":"2024-09-19T04:16:24.581760Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"wb_token = user_secrets.get_secret(\"wandb\")\n\nwandb.login(key=wb_token)\nrun = wandb.init(\n    project='Fine-tune Llama 3 8B for JUnit Test case Generation', \n    job_type=\"training\", \n    anonymous=\"allow\"\n)","metadata":{"id":"na9CAoHC5gM9","execution":{"iopub.status.busy":"2024-09-19T04:16:24.584170Z","iopub.execute_input":"2024-09-19T04:16:24.585028Z","iopub.status.idle":"2024-09-19T04:16:28.631206Z","shell.execute_reply.started":"2024-09-19T04:16:24.584990Z","shell.execute_reply":"2024-09-19T04:16:28.630266Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwalim\u001b[0m (\u001b[33miHateNLP\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240919_041626-j17sp70h</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/iHateNLP/Fine-tune%20Llama%203%208B%20for%20JUnit%20Test%20case%20Generation/runs/j17sp70h' target=\"_blank\">neat-butterfly-4</a></strong> to <a href='https://wandb.ai/iHateNLP/Fine-tune%20Llama%203%208B%20for%20JUnit%20Test%20case%20Generation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/iHateNLP/Fine-tune%20Llama%203%208B%20for%20JUnit%20Test%20case%20Generation' target=\"_blank\">https://wandb.ai/iHateNLP/Fine-tune%20Llama%203%208B%20for%20JUnit%20Test%20case%20Generation</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/iHateNLP/Fine-tune%20Llama%203%208B%20for%20JUnit%20Test%20case%20Generation/runs/j17sp70h' target=\"_blank\">https://wandb.ai/iHateNLP/Fine-tune%20Llama%203%208B%20for%20JUnit%20Test%20case%20Generation/runs/j17sp70h</a>"},"metadata":{}}]},{"cell_type":"code","source":"base_model = \"meta-llama/Meta-Llama-3.1-8B\"\ndataset_name = \"CodexAI/dataset\"\nnew_model = \"CODEX-Meta-Llama-3.1-8B\"","metadata":{"execution":{"iopub.status.busy":"2024-09-19T04:16:28.633642Z","iopub.execute_input":"2024-09-19T04:16:28.634333Z","iopub.status.idle":"2024-09-19T04:16:28.969644Z","shell.execute_reply.started":"2024-09-19T04:16:28.634285Z","shell.execute_reply":"2024-09-19T04:16:28.968848Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Set torch dtype and attention implementation\nif torch.cuda.get_device_capability()[0] >= 8:\n    !pip install -qqq flash-attn\n    torch_dtype = torch.bfloat16\n    attn_implementation = \"flash_attention_2\"\nelse:\n    torch_dtype = torch.float16\n    attn_implementation = \"eager\"","metadata":{"execution":{"iopub.status.busy":"2024-09-19T04:42:05.698914Z","iopub.execute_input":"2024-09-19T04:42:05.699895Z","iopub.status.idle":"2024-09-19T04:42:06.243815Z","shell.execute_reply.started":"2024-09-19T04:42:05.699849Z","shell.execute_reply":"2024-09-19T04:42:06.242995Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"if torch.cuda.is_available():\n    print(f\"CUDA device: {torch.cuda.get_device_name()}\")\n    device={\"\":0}\n    torch_type=torch.bfloat16\nelse:\n    device=\"cpu\"\n    torch_type=torch.bfloat16\n    print(\"I am begging for mercy already!\")","metadata":{"execution":{"iopub.status.busy":"2024-09-19T04:42:07.959229Z","iopub.execute_input":"2024-09-19T04:42:07.959652Z","iopub.status.idle":"2024-09-19T04:42:08.395664Z","shell.execute_reply.started":"2024-09-19T04:42:07.959610Z","shell.execute_reply":"2024-09-19T04:42:08.394952Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"CUDA device: Tesla P100-PCIE-16GB\n","output_type":"stream"}]},{"cell_type":"code","source":"# QLoRA config\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch_type,\n    bnb_4bit_use_double_quant=True,\n)\n\n# Load model\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model,\n    quantization_config=bnb_config,\n    device_map=device,\n    attn_implementation=attn_implementation,\n)\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)","metadata":{"id":"StJKGiDDHzdk","outputId":"871214ba-6c30-4ecf-ac68-550f296b7ef6","execution":{"iopub.status.busy":"2024-09-19T04:42:09.696795Z","iopub.execute_input":"2024-09-19T04:42:09.697797Z","iopub.status.idle":"2024-09-19T04:44:05.455749Z","shell.execute_reply.started":"2024-09-19T04:42:09.697755Z","shell.execute_reply":"2024-09-19T04:44:05.454873Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/826 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87e380b4be584f728cd16a91282a956f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30ab8d507a364bb481a205d9fb22ab77"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4199d856be7f41738172578b4d3b353a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1350be6743a0413a93be95bcd7539d04"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ec66853d4a94dd38314e52aca617c10"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b1c210a70b34995a757cb60ac64effd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a098de4f4a24f709a41d178d7d0b8bf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3f6d3206f494095bca0be754ca07dbd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/185 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f70ec60d9e854ea18eba45a5f1cbe252"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d2f2a48ed2d4dda93d47c1fd25d8c05"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"105d43934749472c85c0ca308f227e3e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68b4c8d327094c23b53b70d6d1eb7372"}},"metadata":{}}]},{"cell_type":"code","source":"model","metadata":{"execution":{"iopub.status.busy":"2024-09-19T04:44:38.980106Z","iopub.execute_input":"2024-09-19T04:44:38.980488Z","iopub.status.idle":"2024-09-19T04:44:39.379052Z","shell.execute_reply.started":"2024-09-19T04:44:38.980454Z","shell.execute_reply":"2024-09-19T04:44:39.378120Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(128256, 4096)\n    (layers): ModuleList(\n      (0-31): 32 x LlamaDecoderLayer(\n        (self_attn): LlamaAttention(\n          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n      )\n    )\n    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n    (rotary_emb): LlamaRotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n)"},"metadata":{}}]},{"cell_type":"code","source":"model.print_trainable_parameters()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# LoRA config\npeft_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n)\npeft_config","metadata":{"execution":{"iopub.status.busy":"2024-09-19T04:44:40.851291Z","iopub.execute_input":"2024-09-19T04:44:40.851705Z","iopub.status.idle":"2024-09-19T04:44:41.225936Z","shell.execute_reply.started":"2024-09-19T04:44:40.851664Z","shell.execute_reply":"2024-09-19T04:44:41.224918Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='CAUSAL_LM', inference_mode=False, r=16, target_modules={'up_proj', 'down_proj', 'q_proj', 'o_proj', 'gate_proj', 'v_proj', 'k_proj'}, lora_alpha=32, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False))"},"metadata":{}}]},{"cell_type":"code","source":"model, tokenizer = setup_chat_format(model, tokenizer)\nmodel = get_peft_model(model, peft_config)\nmodel","metadata":{"execution":{"iopub.status.busy":"2024-09-19T04:44:42.557059Z","iopub.execute_input":"2024-09-19T04:44:42.557903Z","iopub.status.idle":"2024-09-19T04:44:43.862533Z","shell.execute_reply.started":"2024-09-19T04:44:42.557843Z","shell.execute_reply":"2024-09-19T04:44:43.861545Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"model.print_trainable_parameters()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = load_dataset(dataset_name)\ndataset","metadata":{"execution":{"iopub.status.busy":"2024-09-19T04:44:44.798389Z","iopub.execute_input":"2024-09-19T04:44:44.799270Z","iopub.status.idle":"2024-09-19T04:44:49.006709Z","shell.execute_reply.started":"2024-09-19T04:44:44.799226Z","shell.execute_reply":"2024-09-19T04:44:49.005636Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/426 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5dadb6fae4664ffb805c91ff84bf1dc0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/35.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e7813c9d79e4a14977ade52d8a72ae7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/1.74M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c3d64fccbe54b6a9aed8cfe21247883"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/100000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83212dc739064d4590926f9b2390ea3b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/4687 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f69909244274c2fbb2be1ea10a7e4dc"}},"metadata":{}},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['focal_method', 'test_case'],\n        num_rows: 100000\n    })\n    test: Dataset({\n        features: ['focal_method', 'test_case'],\n        num_rows: 4687\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"train=dataset['train']\ntrain=train.shuffle(True).select(range(1000))\ntrain","metadata":{"execution":{"iopub.status.busy":"2024-09-19T04:44:50.571049Z","iopub.execute_input":"2024-09-19T04:44:50.571445Z","iopub.status.idle":"2024-09-19T04:44:51.063160Z","shell.execute_reply.started":"2024-09-19T04:44:50.571406Z","shell.execute_reply":"2024-09-19T04:44:51.062400Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['focal_method', 'test_case'],\n    num_rows: 1000\n})"},"metadata":{}}]},{"cell_type":"code","source":"test=dataset['test']\ntest=test.shuffle(True).select(range(100))\ntest","metadata":{"execution":{"iopub.status.busy":"2024-09-19T04:44:54.535041Z","iopub.execute_input":"2024-09-19T04:44:54.535471Z","iopub.status.idle":"2024-09-19T04:44:54.950098Z","shell.execute_reply.started":"2024-09-19T04:44:54.535430Z","shell.execute_reply":"2024-09-19T04:44:54.949338Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['focal_method', 'test_case'],\n    num_rows: 100\n})"},"metadata":{}}]},{"cell_type":"code","source":"def format_chat_template(row):\n    row_json = [\n        {\"role\": \"system\", \"content\": \"You are a coding assistant for generating JUnit test cases.\"},\n        {\"role\": \"user\", \"content\": row[\"focal_method\"]},\n        {\"role\": \"assistant\", \"content\": row[\"test_case\"]}\n    ]\n    row[\"text\"] = tokenizer.apply_chat_template(row_json, tokenize=False)\n    return row\n","metadata":{"execution":{"iopub.status.busy":"2024-09-19T04:44:56.408719Z","iopub.execute_input":"2024-09-19T04:44:56.409086Z","iopub.status.idle":"2024-09-19T04:44:56.780411Z","shell.execute_reply.started":"2024-09-19T04:44:56.409052Z","shell.execute_reply":"2024-09-19T04:44:56.779631Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"train = train.map(\n    format_chat_template,\n    num_proc= 4,\n)\n\ntrain","metadata":{"id":"XzF2UjPvTBag","outputId":"3733e45f-605e-4564-88c7-368c9c5bf9cd","execution":{"iopub.status.busy":"2024-09-19T04:45:00.665682Z","iopub.execute_input":"2024-09-19T04:45:00.666101Z","iopub.status.idle":"2024-09-19T04:45:01.711379Z","shell.execute_reply.started":"2024-09-19T04:45:00.666062Z","shell.execute_reply":"2024-09-19T04:45:01.710520Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36f43d4d50864b76a9f4310ee83b21d5"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['focal_method', 'test_case', 'text'],\n    num_rows: 1000\n})"},"metadata":{}}]},{"cell_type":"code","source":"test = test.map(\n    format_chat_template,\n    num_proc= 4,\n)\n\ntest","metadata":{"execution":{"iopub.status.busy":"2024-09-19T04:45:10.287035Z","iopub.execute_input":"2024-09-19T04:45:10.287451Z","iopub.status.idle":"2024-09-19T04:45:11.359820Z","shell.execute_reply.started":"2024-09-19T04:45:10.287411Z","shell.execute_reply":"2024-09-19T04:45:11.358984Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f63cee47a97a443fac0f832f2dc3a8f0"}},"metadata":{}},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['focal_method', 'test_case', 'text'],\n    num_rows: 100\n})"},"metadata":{}}]},{"cell_type":"code","source":"train['text'][3]","metadata":{"execution":{"iopub.status.busy":"2024-09-19T04:45:15.203671Z","iopub.execute_input":"2024-09-19T04:45:15.204077Z","iopub.status.idle":"2024-09-19T04:45:15.678043Z","shell.execute_reply.started":"2024-09-19T04:45:15.204034Z","shell.execute_reply":"2024-09-19T04:45:15.677035Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"'<|im_start|>user\\npublic OutputStream createRawOutputStream() throws IOException\\n    {\\n        checkClosed();\\n        if (isWriting)\\n        {\\n            throw new IllegalStateException(\"Cannot have more than one open stream writer.\");\\n        }\\n        if (randomAccess != null)\\n            randomAccess.clear();\\n        else\\n            randomAccess = getStreamCache().createBuffer();\\n        OutputStream out = new RandomAccessOutputStream(randomAccess);\\n        isWriting = true;\\n        return new FilterOutputStream(out)\\n        {\\n            @Override\\n            public void write(byte[] b, int off, int len) throws IOException\\n            {\\n                this.out.write(b, off, len);\\n            }\\n            \\n            @Override\\n            public void close() throws IOException\\n            {\\n                super.close();\\n                setInt(COSName.LENGTH, (int)randomAccess.length());\\n                isWriting = false;\\n            }\\n        };\\n    }<|im_end|>\\n<|im_start|>assistant\\n@Test\\n    void testCompressedStream1Decode() throws IOException\\n    {\\n        byte[] testString = \"This is a test string to be used as input for TestCOSStream\".getBytes(StandardCharsets.US_ASCII);\\n        byte[] testStringEncoded = encodeData(testString, COSName.FLATE_DECODE);\\n        COSStream stream = new COSStream();\\n        \\n        try (OutputStream output = stream.createRawOutputStream())\\n        {\\n            output.write(testStringEncoded);\\n        }\\n\\n        stream.setItem(COSName.FILTER, COSName.FLATE_DECODE);\\n        validateDecoded(stream, testString);\\n    }<|im_end|>\\n'"},"metadata":{}}]},{"cell_type":"code","source":"#Hyperparamter\ntraining_arguments = TrainingArguments(\n    output_dir=\"./results\",\n    run_name =\"./loggings\",\n    overwrite_output_dir=True,\n    eval_strategy=\"steps\",\n    eval_steps=0.10,\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    gradient_accumulation_steps=2,\n    optim=\"paged_adamw_32bit\",\n    num_train_epochs=1,\n    warmup_steps=10,\n    logging_strategy=\"steps\",\n    logging_steps=50,\n    logging_first_step=True,\n    learning_rate=2e-4,\n    fp16=False,\n    bf16=True,\n    group_by_length=True,\n    report_to=\"wandb\",\n)","metadata":{"id":"peOnLAAhS0y1","execution":{"iopub.status.busy":"2024-09-19T04:46:01.792151Z","iopub.execute_input":"2024-09-19T04:46:01.792855Z","iopub.status.idle":"2024-09-19T04:46:02.234758Z","shell.execute_reply.started":"2024-09-19T04:46:01.792811Z","shell.execute_reply":"2024-09-19T04:46:02.233968Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# Setting sft parameters\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=train,\n    eval_dataset=test,\n    peft_config=peft_config,\n    max_seq_length= 512,\n    dataset_text_field=\"text\",\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing= False,\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-19T04:46:03.460448Z","iopub.execute_input":"2024-09-19T04:46:03.460853Z","iopub.status.idle":"2024-09-19T04:46:05.483825Z","shell.execute_reply.started":"2024-09-19T04:46:03.460812Z","shell.execute_reply":"2024-09-19T04:46:05.483048Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length, dataset_text_field. Will not be supported from version '1.0.0'.\n\nDeprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n  warnings.warn(message, FutureWarning)\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:321: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7735a832a8d49239d5853a23fad0f1b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ddc1ce4dba024e75ae0a633d30c04aa1"}},"metadata":{}}]},{"cell_type":"code","source":"trainer.train_dataset","metadata":{"execution":{"iopub.status.busy":"2024-09-19T06:03:49.019492Z","iopub.execute_input":"2024-09-19T06:03:49.019910Z","iopub.status.idle":"2024-09-19T06:03:49.026495Z","shell.execute_reply.started":"2024-09-19T06:03:49.019871Z","shell.execute_reply":"2024-09-19T06:03:49.025643Z"},"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['input_ids', 'attention_mask'],\n    num_rows: 1000\n})"},"metadata":{}}]},{"cell_type":"code","source":"trainer.eval_dataset","metadata":{"execution":{"iopub.status.busy":"2024-09-19T06:03:52.168469Z","iopub.execute_input":"2024-09-19T06:03:52.169361Z","iopub.status.idle":"2024-09-19T06:03:52.175404Z","shell.execute_reply.started":"2024-09-19T06:03:52.169320Z","shell.execute_reply":"2024-09-19T06:03:52.174444Z"},"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['input_ids', 'attention_mask'],\n    num_rows: 100\n})"},"metadata":{}}]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-09-19T04:46:10.014309Z","iopub.execute_input":"2024-09-19T04:46:10.015026Z","iopub.status.idle":"2024-09-19T05:56:20.431529Z","shell.execute_reply.started":"2024-09-19T04:46:10.014983Z","shell.execute_reply":"2024-09-19T05:56:20.430664Z"},"trusted":true},"execution_count":21,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [500/500 1:09:58, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>50</td>\n      <td>1.371500</td>\n      <td>1.268404</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>1.392400</td>\n      <td>1.265884</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>1.266900</td>\n      <td>1.245193</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.410900</td>\n      <td>1.233470</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>1.302100</td>\n      <td>1.223866</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>1.415900</td>\n      <td>1.220015</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>1.223900</td>\n      <td>1.212525</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>1.306600</td>\n      <td>1.210337</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>1.307100</td>\n      <td>1.203429</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>1.199800</td>\n      <td>1.201871</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:232: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=500, training_loss=1.267729706287384, metrics={'train_runtime': 4208.3025, 'train_samples_per_second': 0.238, 'train_steps_per_second': 0.119, 'total_flos': 1.1397699521519616e+16, 'train_loss': 1.267729706287384, 'epoch': 1.0})"},"metadata":{}}]},{"cell_type":"code","source":"trainer.model","metadata":{"execution":{"iopub.status.busy":"2024-09-19T05:57:35.405091Z","iopub.execute_input":"2024-09-19T05:57:35.406018Z","iopub.status.idle":"2024-09-19T05:57:35.883474Z","shell.execute_reply.started":"2024-09-19T05:57:35.405973Z","shell.execute_reply":"2024-09-19T05:57:35.882555Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): LlamaForCausalLM(\n      (model): LlamaModel(\n        (embed_tokens): Embedding(128258, 4096)\n        (layers): ModuleList(\n          (0-31): 32 x LlamaDecoderLayer(\n            (self_attn): LlamaAttention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (rotary_emb): LlamaRotaryEmbedding()\n            )\n            (mlp): LlamaMLP(\n              (gate_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=14336, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (up_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=14336, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (down_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=14336, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (act_fn): SiLU()\n            )\n            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n          )\n        )\n        (norm): LlamaRMSNorm((4096,), eps=1e-05)\n        (rotary_emb): LlamaRotaryEmbedding()\n      )\n      (lm_head): Linear(in_features=4096, out_features=128258, bias=False)\n    )\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"trainer.model.save_pretrained(new_model)\nwandb.finish()\nmodel.config.use_cache = True","metadata":{"id":"nKgZBEGVS5a2","execution":{"iopub.status.busy":"2024-09-19T05:57:55.494690Z","iopub.execute_input":"2024-09-19T05:57:55.495078Z","iopub.status.idle":"2024-09-19T05:58:08.391384Z","shell.execute_reply.started":"2024-09-19T05:57:55.495039Z","shell.execute_reply":"2024-09-19T05:58:08.390421Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:232: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='2.604 MB of 2.604 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>██▆▄▃▃▂▂▁▁</td></tr><tr><td>eval/runtime</td><td>█▄▆▄▁▄▄▄▅▄</td></tr><tr><td>eval/samples_per_second</td><td>▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/steps_per_second</td><td>▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/grad_norm</td><td>▂▂▃▂▄▂▅▁▅▃▃▂▅▂▄▃▄▃▄▁█</td></tr><tr><td>train/learning_rate</td><td>▂██▇▇▇▆▆▅▅▅▄▄▄▃▃▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>█▆▇▂▇▃▄▂█▃▅▁█▁▃▄▅▁▅▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.20187</td></tr><tr><td>eval/runtime</td><td>137.9093</td></tr><tr><td>eval/samples_per_second</td><td>0.725</td></tr><tr><td>eval/steps_per_second</td><td>0.725</td></tr><tr><td>total_flos</td><td>1.1397699521519616e+16</td></tr><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/global_step</td><td>500</td></tr><tr><td>train/grad_norm</td><td>3.47254</td></tr><tr><td>train/learning_rate</td><td>0</td></tr><tr><td>train/loss</td><td>1.1998</td></tr><tr><td>train_loss</td><td>1.26773</td></tr><tr><td>train_runtime</td><td>4208.3025</td></tr><tr><td>train_samples_per_second</td><td>0.238</td></tr><tr><td>train_steps_per_second</td><td>0.119</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">neat-butterfly-4</strong> at: <a href='https://wandb.ai/iHateNLP/Fine-tune%20Llama%203%208B%20for%20JUnit%20Test%20case%20Generation/runs/j17sp70h' target=\"_blank\">https://wandb.ai/iHateNLP/Fine-tune%20Llama%203%208B%20for%20JUnit%20Test%20case%20Generation/runs/j17sp70h</a><br/> View project at: <a href='https://wandb.ai/iHateNLP/Fine-tune%20Llama%203%208B%20for%20JUnit%20Test%20case%20Generation' target=\"_blank\">https://wandb.ai/iHateNLP/Fine-tune%20Llama%203%208B%20for%20JUnit%20Test%20case%20Generation</a><br/>Synced 5 W&B file(s), 0 media file(s), 19 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240919_041626-j17sp70h/logs</code>"},"metadata":{}}]},{"cell_type":"code","source":"messages = [{\"role\": \"user\", \"content\": \"\"\"\npublic class SimpleCalculator {\n    // Method to add two numbers\n    public int add(int a, int b) {\n        return a + b;\n    }\n\n    // Method to subtract two numbers\n    public int subtract(int a, int b) {\n        return a - b;\n    }\n\n    // Method to multiply two numbers\n    public int multiply(int a, int b) {\n        return a * b;\n    }\n\n    // Method to divide two numbers\n    // Throws ArithmeticException if divisor is zero\n    public double divide(int a, int b) {\n        if (b == 0) {\n            throw new ArithmeticException(\"Cannot divide by zero\");\n        }\n        return (double) a / b;\n    }\n}\n\"\"\"}]\n\nprompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    \ninputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n\noutputs = model.generate(**inputs, max_new_tokens=512, num_return_sequences=1)\n\ntext = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\nprint(text.split(\"assistant\")[1])","metadata":{"id":"L7vHP41ITQPb","execution":{"iopub.status.busy":"2024-09-19T05:59:35.722261Z","iopub.execute_input":"2024-09-19T05:59:35.722671Z","iopub.status.idle":"2024-09-19T06:00:48.870225Z","shell.execute_reply.started":"2024-09-19T05:59:35.722625Z","shell.execute_reply":"2024-09-19T06:00:48.869229Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"\n@Test\n    public void testAdd() {\n        assertEquals(5, calculator.add(2, 3));\n    }\n","output_type":"stream"}]},{"cell_type":"code","source":"from huggingface_hub import HfApi, create_repo","metadata":{"execution":{"iopub.status.busy":"2024-09-19T06:01:54.790552Z","iopub.execute_input":"2024-09-19T06:01:54.791353Z","iopub.status.idle":"2024-09-19T06:01:54.795976Z","shell.execute_reply.started":"2024-09-19T06:01:54.791310Z","shell.execute_reply":"2024-09-19T06:01:54.794951Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"repo_name = new_model\norganization_name = \"CodexAI\"\nrepo_url = f\"{organization_name}/{repo_name}\"","metadata":{"execution":{"iopub.status.busy":"2024-09-19T06:01:59.424193Z","iopub.execute_input":"2024-09-19T06:01:59.424828Z","iopub.status.idle":"2024-09-19T06:01:59.429209Z","shell.execute_reply.started":"2024-09-19T06:01:59.424785Z","shell.execute_reply":"2024-09-19T06:01:59.428230Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"create_repo(repo_url, repo_type=\"model\", private=True,exist_ok=True)","metadata":{"execution":{"iopub.status.busy":"2024-09-19T06:02:07.897558Z","iopub.execute_input":"2024-09-19T06:02:07.898480Z","iopub.status.idle":"2024-09-19T06:02:08.442617Z","shell.execute_reply.started":"2024-09-19T06:02:07.898437Z","shell.execute_reply":"2024-09-19T06:02:08.441603Z"},"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"RepoUrl('https://huggingface.co/CodexAI/CODEX-Meta-Llama-3.1-8B', endpoint='https://huggingface.co', repo_type='model', repo_id='CodexAI/CODEX-Meta-Llama-3.1-8B')"},"metadata":{}}]},{"cell_type":"code","source":"api = HfApi()\napi.upload_folder(folder_path=new_model,repo_id=repo_url)","metadata":{"execution":{"iopub.status.busy":"2024-09-19T06:02:14.321816Z","iopub.execute_input":"2024-09-19T06:02:14.322187Z","iopub.status.idle":"2024-09-19T06:03:24.437025Z","shell.execute_reply.started":"2024-09-19T06:02:14.322154Z","shell.execute_reply":"2024-09-19T06:03:24.436042Z"},"trusted":true},"execution_count":29,"outputs":[{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/2.27G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85825171b6034df9b0f5e593f405896b"}},"metadata":{}},{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/CodexAI/CODEX-Meta-Llama-3.1-8B/commit/acee6c73fad1c5e63e5d56c0abbd60c592fcd2ed', commit_message='Upload folder using huggingface_hub', commit_description='', oid='acee6c73fad1c5e63e5d56c0abbd60c592fcd2ed', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}]},{"cell_type":"code","source":"print('END')","metadata":{"execution":{"iopub.status.busy":"2024-09-19T06:04:14.695225Z","iopub.execute_input":"2024-09-19T06:04:14.695949Z","iopub.status.idle":"2024-09-19T06:04:14.700875Z","shell.execute_reply.started":"2024-09-19T06:04:14.695908Z","shell.execute_reply":"2024-09-19T06:04:14.699863Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"END\n","output_type":"stream"}]}]}