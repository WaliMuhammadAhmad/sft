{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture \n%pip install -U accelerate peft bitsandbytes transformers trl evaluate attrdict tqdm datasets tensorboardX","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-04T11:58:01.199235Z","iopub.execute_input":"2024-09-04T11:58:01.199561Z","iopub.status.idle":"2024-09-04T11:58:35.380301Z","shell.execute_reply.started":"2024-09-04T11:58:01.199527Z","shell.execute_reply":"2024-09-04T11:58:35.378963Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\nimport json\nimport shutil\nimport logging\nimport transformers\n\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    TrainingArguments,\n    pipeline,\n    logging,\n)\nfrom peft import (\n    LoraConfig,\n    get_peft_model,\n    TaskType,\n    PeftModel, \n    PeftConfig\n)\nfrom evaluate import load\nfrom trl import SFTTrainer\nfrom datasets import Dataset, load_dataset\nfrom huggingface_hub import login, Repository","metadata":{"execution":{"iopub.status.busy":"2024-09-04T11:58:35.382624Z","iopub.execute_input":"2024-09-04T11:58:35.383030Z","iopub.status.idle":"2024-09-04T11:58:55.214036Z","shell.execute_reply.started":"2024-09-04T11:58:35.382985Z","shell.execute_reply":"2024-09-04T11:58:55.213204Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"model = \"deepseek-coder-1.3b-instruct\"  # model to be fine-tuned\n\nbase_model = \"deepseek-ai/deepseek-coder-1.3b-instruct\"  # actual model name\n\nnew_model = \"CODEX-deepseek-coder-1.3b-instruct\"  # fine-tunned model name\n\ndataset_path = \"dataset\"  # dataset dir path\n\ndataset = \"CodexAI/Eval4Deepseek-Coder\"  # dataset name at huggingface\n\nrepo_url = f'https://huggingface.co/datasets/{dataset}'","metadata":{"execution":{"iopub.status.busy":"2024-09-04T12:00:24.704677Z","iopub.execute_input":"2024-09-04T12:00:24.705464Z","iopub.status.idle":"2024-09-04T12:00:24.711009Z","shell.execute_reply.started":"2024-09-04T12:00:24.705422Z","shell.execute_reply":"2024-09-04T12:00:24.709892Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"if not os.path.exists(dataset_path):\n    os.makedirs(dataset_path)","metadata":{"execution":{"iopub.status.busy":"2024-09-04T12:00:25.685112Z","iopub.execute_input":"2024-09-04T12:00:25.685764Z","iopub.status.idle":"2024-09-04T12:00:25.690111Z","shell.execute_reply.started":"2024-09-04T12:00:25.685715Z","shell.execute_reply":"2024-09-04T12:00:25.689151Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nlogin(UserSecretsClient().get_secret(\"HF_TOKEN\"))","metadata":{"execution":{"iopub.status.busy":"2024-09-04T12:00:28.203653Z","iopub.execute_input":"2024-09-04T12:00:28.204265Z","iopub.status.idle":"2024-09-04T12:00:28.532067Z","shell.execute_reply.started":"2024-09-04T12:00:28.204225Z","shell.execute_reply":"2024-09-04T12:00:28.531161Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"# repo = Repository(local_dir=dataset_path,clone_from=repo_url)","metadata":{"execution":{"iopub.status.busy":"2024-09-04T12:00:46.061461Z","iopub.execute_input":"2024-09-04T12:00:46.062348Z","iopub.status.idle":"2024-09-04T12:00:46.066480Z","shell.execute_reply.started":"2024-09-04T12:00:46.062308Z","shell.execute_reply":"2024-09-04T12:00:46.065352Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def load_json_data(dir_name):\n\n  data=[]\n  for root_folder in os.listdir(dir_name):\n    if root_folder!=\".git\" and root_folder!=\".gitattributes\":\n      for files in os.listdir(os.path.join(dir_name,root_folder)):\n        if files.endswith(\".json\"):\n          with open(os.path.join(dir_name,root_folder,files),\"r\")as f:\n            json_file=json.load(f)\n            data.append(json_file)\n  return data","metadata":{"execution":{"iopub.status.busy":"2024-09-04T12:00:53.441361Z","iopub.execute_input":"2024-09-04T12:00:53.442188Z","iopub.status.idle":"2024-09-04T12:00:53.448399Z","shell.execute_reply.started":"2024-09-04T12:00:53.442148Z","shell.execute_reply":"2024-09-04T12:00:53.447324Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"print(f\"Loading dataset from ./{dataset_path}/...\")\njson_data=load_json_data(dataset_path)","metadata":{"execution":{"iopub.status.busy":"2024-09-04T12:00:54.754247Z","iopub.execute_input":"2024-09-04T12:00:54.754853Z","iopub.status.idle":"2024-09-04T12:00:58.672281Z","shell.execute_reply.started":"2024-09-04T12:00:54.754814Z","shell.execute_reply":"2024-09-04T12:00:58.671266Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Loading dataset from /dataset/...\n","output_type":"stream"}]},{"cell_type":"code","source":"print(f\"Length of loaded dataset is: {len(json_data)}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-04T12:01:09.570861Z","iopub.execute_input":"2024-09-04T12:01:09.571254Z","iopub.status.idle":"2024-09-04T12:01:09.576492Z","shell.execute_reply.started":"2024-09-04T12:01:09.571213Z","shell.execute_reply":"2024-09-04T12:01:09.575550Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Length of loaded dataset is: 78534\n","output_type":"stream"}]},{"cell_type":"code","source":"tmp=json_data  # in case if this is required again","metadata":{"execution":{"iopub.status.busy":"2024-09-04T12:01:11.969805Z","iopub.execute_input":"2024-09-04T12:01:11.970671Z","iopub.status.idle":"2024-09-04T12:01:11.975288Z","shell.execute_reply.started":"2024-09-04T12:01:11.970625Z","shell.execute_reply":"2024-09-04T12:01:11.974091Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"json_data=json_data[:1000]\nprint(f\"Length of dataset is: {len(json_data)}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-04T12:01:13.630417Z","iopub.execute_input":"2024-09-04T12:01:13.631110Z","iopub.status.idle":"2024-09-04T12:01:13.635824Z","shell.execute_reply.started":"2024-09-04T12:01:13.631067Z","shell.execute_reply":"2024-09-04T12:01:13.634868Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Length of dataset is: 1000\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\"Loading dataset...\")\ndf=Dataset.from_list(json_data)","metadata":{"execution":{"iopub.status.busy":"2024-09-04T12:01:15.380272Z","iopub.execute_input":"2024-09-04T12:01:15.380660Z","iopub.status.idle":"2024-09-04T12:01:15.435627Z","shell.execute_reply.started":"2024-09-04T12:01:15.380622Z","shell.execute_reply":"2024-09-04T12:01:15.434751Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Loading dataset...\n","output_type":"stream"}]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2024-09-04T12:01:16.654696Z","iopub.execute_input":"2024-09-04T12:01:16.655522Z","iopub.status.idle":"2024-09-04T12:01:16.664035Z","shell.execute_reply.started":"2024-09-04T12:01:16.655473Z","shell.execute_reply":"2024-09-04T12:01:16.663103Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['instruction', 'output'],\n    num_rows: 1000\n})"},"metadata":{}}]},{"cell_type":"code","source":"df.features","metadata":{"execution":{"iopub.status.busy":"2024-09-04T12:01:23.138498Z","iopub.execute_input":"2024-09-04T12:01:23.139201Z","iopub.status.idle":"2024-09-04T12:01:23.145454Z","shell.execute_reply.started":"2024-09-04T12:01:23.139161Z","shell.execute_reply":"2024-09-04T12:01:23.144467Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"{'instruction': Value(dtype='string', id=None),\n 'output': Value(dtype='string', id=None)}"},"metadata":{}}]},{"cell_type":"code","source":"df['instruction'][0]","metadata":{"execution":{"iopub.status.busy":"2024-09-04T12:01:31.138458Z","iopub.execute_input":"2024-09-04T12:01:31.139317Z","iopub.status.idle":"2024-09-04T12:01:31.151413Z","shell.execute_reply.started":"2024-09-04T12:01:31.139274Z","shell.execute_reply":"2024-09-04T12:01:31.150518Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"'Generate a unit test case for the following Java method: SourceFileResolver { public static File resolveSourceFile( List<String> sourcePaths, String sourceFileName, String groupId, String artifactId ) { return resolveSourceFile( sourceFileName, PathUtil.filesList( sourcePaths ), groupId, artifactId ); }  static File resolveSourceFile( List<String> sourcePaths, String sourceFileName, String groupId,\\n                                          String artifactId ); static File resolveSourceFile( List<String> sourcePaths, String sourceFile ); static File resolveSourceFile( String sourceFileName, List<File> sourceRoots ); static File resolveSourceFile( String sourceFileName, List<File> sourceRoots, String groupId,\\n                                          String artifactId );  }'"},"metadata":{}}]},{"cell_type":"code","source":"df['output'][0]","metadata":{"execution":{"iopub.status.busy":"2024-09-04T12:01:32.320758Z","iopub.execute_input":"2024-09-04T12:01:32.321388Z","iopub.status.idle":"2024-09-04T12:01:32.329607Z","shell.execute_reply.started":"2024-09-04T12:01:32.321345Z","shell.execute_reply":"2024-09-04T12:01:32.328595Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"'The unit test case for the given Java method is: @Test public void resolveMultipleRoots() { File file = SourceFileResolver.resolveSourceFile( null, getDir( \"nroots/root1\", \"nroots/root2\", \"nroots/root3\" ), null, null ); Assert.assertTrue( file.exists() ); MatcherAssert.assertThat( file.getName(), CoreMatchers.equalTo( \"root.as\" ) ); }'"},"metadata":{}}]},{"cell_type":"code","source":"print(\"Spliting dataset...\")\ndf=df.train_test_split(test_size=0.2)","metadata":{"execution":{"iopub.status.busy":"2024-09-04T12:01:35.719310Z","iopub.execute_input":"2024-09-04T12:01:35.720193Z","iopub.status.idle":"2024-09-04T12:01:35.746045Z","shell.execute_reply.started":"2024-09-04T12:01:35.720150Z","shell.execute_reply":"2024-09-04T12:01:35.744971Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Spliting dataset...\n","output_type":"stream"}]},{"cell_type":"code","source":"print(df)","metadata":{"execution":{"iopub.status.busy":"2024-09-04T12:01:39.838463Z","iopub.execute_input":"2024-09-04T12:01:39.839253Z","iopub.status.idle":"2024-09-04T12:01:39.844072Z","shell.execute_reply.started":"2024-09-04T12:01:39.839210Z","shell.execute_reply":"2024-09-04T12:01:39.843092Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['instruction', 'output'],\n        num_rows: 800\n    })\n    test: Dataset({\n        features: ['instruction', 'output'],\n        num_rows: 200\n    })\n})\n","output_type":"stream"}]},{"cell_type":"code","source":"train=df['train']\ntest=df['test']","metadata":{"execution":{"iopub.status.busy":"2024-09-04T12:01:51.227127Z","iopub.execute_input":"2024-09-04T12:01:51.227531Z","iopub.status.idle":"2024-09-04T12:01:51.232422Z","shell.execute_reply.started":"2024-09-04T12:01:51.227492Z","shell.execute_reply":"2024-09-04T12:01:51.231226Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"train","metadata":{"execution":{"iopub.status.busy":"2024-09-04T12:01:52.625913Z","iopub.execute_input":"2024-09-04T12:01:52.626322Z","iopub.status.idle":"2024-09-04T12:01:52.632735Z","shell.execute_reply.started":"2024-09-04T12:01:52.626284Z","shell.execute_reply":"2024-09-04T12:01:52.631814Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['instruction', 'output'],\n    num_rows: 800\n})"},"metadata":{}}]},{"cell_type":"code","source":"test","metadata":{"execution":{"iopub.status.busy":"2024-09-04T12:02:01.101961Z","iopub.execute_input":"2024-09-04T12:02:01.102333Z","iopub.status.idle":"2024-09-04T12:02:01.108557Z","shell.execute_reply.started":"2024-09-04T12:02:01.102299Z","shell.execute_reply":"2024-09-04T12:02:01.107529Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['instruction', 'output'],\n    num_rows: 200\n})"},"metadata":{}}]},{"cell_type":"code","source":"print(\"Loading tokenizer...\")\ntokenizer = AutoTokenizer.from_pretrained(base_model)","metadata":{"execution":{"iopub.status.busy":"2024-09-04T12:02:02.536496Z","iopub.execute_input":"2024-09-04T12:02:02.537217Z","iopub.status.idle":"2024-09-04T12:02:04.051296Z","shell.execute_reply.started":"2024-09-04T12:02:02.537178Z","shell.execute_reply":"2024-09-04T12:02:04.050310Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Loading tokenizer...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.87k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2059ab6b2c6f4049ae8127d18c98bbef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.37M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d4eab2096ef4a92ba94447adafcf4cf"}},"metadata":{}}]},{"cell_type":"code","source":"instruction = tokenizer(train['instruction'][0])\nprint(instruction)","metadata":{"execution":{"iopub.status.busy":"2024-09-04T12:02:07.765799Z","iopub.execute_input":"2024-09-04T12:02:07.766172Z","iopub.status.idle":"2024-09-04T12:02:07.784996Z","shell.execute_reply.started":"2024-09-04T12:02:07.766137Z","shell.execute_reply":"2024-09-04T12:02:07.784062Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"{'input_ids': [32013, 7605, 387, 245, 5621, 1719, 1452, 327, 254, 1884, 9840, 2040, 25, 380, 6327, 508, 7207, 507, 1171, 12353, 7928, 1270, 3314, 1013, 51, 8121, 380, 6327, 508, 29, 323, 3341, 938, 10647, 7, 10647, 270, 8, 8474, 13147, 10647, 508, 3305, 507, 967, 3341, 938, 10647, 7, 65, 13, 24657, 7, 10647, 508, 13, 6787, 57, 650, 270, 477, 611, 207, 1171, 12353, 7928, 3314, 323, 3341, 938, 10647, 7, 10647, 270, 477, 1171, 12353, 7928, 1171, 6159, 1476, 54, 1661, 787, 1195, 309, 19791, 2456, 3314, 323, 3341, 938, 10647, 7, 3667, 27, 51, 29, 495, 82, 11, 380, 6327, 270, 477, 1171, 12353, 7928, 1171, 6159, 1476, 54, 1661, 787, 1195, 309, 19791, 2456, 3314, 323, 3341, 938, 10647, 7, 2005, 1208, 11, 380, 6327, 270, 477, 207, 611], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n","output_type":"stream"}]},{"cell_type":"code","source":"tokens = tokenizer.convert_ids_to_tokens(instruction.input_ids)\nprint(tokens)","metadata":{"execution":{"iopub.status.busy":"2024-09-04T12:02:11.641461Z","iopub.execute_input":"2024-09-04T12:02:11.641864Z","iopub.status.idle":"2024-09-04T12:02:11.646923Z","shell.execute_reply.started":"2024-09-04T12:02:11.641827Z","shell.execute_reply":"2024-09-04T12:02:11.646011Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"['<｜begin▁of▁sentence｜>', 'Gener', 'ate', 'Ġa', 'Ġunit', 'Ġtest', 'Ġcase', 'Ġfor', 'Ġthe', 'Ġfollowing', 'ĠJava', 'Ġmethod', ':', 'ĠB', 'undle', 'able', 'Util', 'Ġ{', 'Ġ@', 'Non', 'Null', 'Ġpublic', 'Ġstatic', 'Ġ<', 'T', 'Ġextends', 'ĠB', 'undle', 'able', '>', 'ĠT', 'Ġmaterial', 'ize', 'Bundle', '(', 'Bundle', 'Ġb', ')', 'Ġthrows', 'ĠBad', 'Bundle', 'able', 'Exception', 'Ġ{', 'Ġreturn', 'Ġmaterial', 'ize', 'Bundle', '(', 'b', '.', 'getString', '(', 'Bundle', 'able', '.', 'CL', 'Z', '),', 'Ġb', ');', 'Ġ}', 'Ġ', 'Ġ@', 'Non', 'Null', 'Ġstatic', 'ĠT', 'Ġmaterial', 'ize', 'Bundle', '(', 'Bundle', 'Ġb', ');', 'Ġ@', 'Non', 'Null', 'Ġ@', 'Sup', 'press', 'W', 'arn', 'ings', '(\"', 'un', 'checked', '\")', 'Ġstatic', 'ĠT', 'Ġmaterial', 'ize', 'Bundle', '(', 'Class', '<', 'T', '>', 'Ġcl', 's', ',', 'ĠB', 'undle', 'Ġb', ');', 'Ġ@', 'Non', 'Null', 'Ġ@', 'Sup', 'press', 'W', 'arn', 'ings', '(\"', 'un', 'checked', '\")', 'Ġstatic', 'ĠT', 'Ġmaterial', 'ize', 'Bundle', '(', 'String', 'Ġname', ',', 'ĠB', 'undle', 'Ġb', ');', 'Ġ', 'Ġ}']\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenizer.convert_tokens_to_string(tokens)","metadata":{"execution":{"iopub.status.busy":"2024-09-04T12:02:16.863567Z","iopub.execute_input":"2024-09-04T12:02:16.863996Z","iopub.status.idle":"2024-09-04T12:02:16.870977Z","shell.execute_reply.started":"2024-09-04T12:02:16.863955Z","shell.execute_reply":"2024-09-04T12:02:16.870056Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"'<｜begin▁of▁sentence｜>Generate a unit test case for the following Java method: BundleableUtil { @NonNull public static <T extends Bundleable> T materializeBundle(Bundle b) throws BadBundleableException { return materializeBundle(b.getString(Bundleable.CLZ), b); }  @NonNull static T materializeBundle(Bundle b); @NonNull @SuppressWarnings(\"unchecked\") static T materializeBundle(Class<T> cls, Bundle b); @NonNull @SuppressWarnings(\"unchecked\") static T materializeBundle(String name, Bundle b);  }'"},"metadata":{}}]},{"cell_type":"code","source":"print(f\"Vocab size : {tokenizer.vocab_size}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-04T12:02:23.605406Z","iopub.execute_input":"2024-09-04T12:02:23.606097Z","iopub.status.idle":"2024-09-04T12:02:23.610749Z","shell.execute_reply.started":"2024-09-04T12:02:23.606057Z","shell.execute_reply":"2024-09-04T12:02:23.609790Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"Vocab size : 32000\n","output_type":"stream"}]},{"cell_type":"code","source":"print(f\"max length : {tokenizer.model_max_length}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-04T12:02:26.796456Z","iopub.execute_input":"2024-09-04T12:02:26.797449Z","iopub.status.idle":"2024-09-04T12:02:26.802666Z","shell.execute_reply.started":"2024-09-04T12:02:26.797400Z","shell.execute_reply":"2024-09-04T12:02:26.801432Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"max length : 16384\n","output_type":"stream"}]},{"cell_type":"code","source":"print(f\"model input : {tokenizer.model_input_names}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-04T12:02:32.229035Z","iopub.execute_input":"2024-09-04T12:02:32.229422Z","iopub.status.idle":"2024-09-04T12:02:32.234109Z","shell.execute_reply.started":"2024-09-04T12:02:32.229384Z","shell.execute_reply":"2024-09-04T12:02:32.233218Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"model input : ['input_ids', 'attention_mask']\n","output_type":"stream"}]},{"cell_type":"code","source":"batch = tokenizer(train['instruction'][0],max_length=512,truncation=True,padding=\"max_length\",return_tensors=\"pt\")","metadata":{"execution":{"iopub.status.busy":"2024-09-04T12:02:41.219735Z","iopub.execute_input":"2024-09-04T12:02:41.220694Z","iopub.status.idle":"2024-09-04T12:02:41.236339Z","shell.execute_reply.started":"2024-09-04T12:02:41.220636Z","shell.execute_reply":"2024-09-04T12:02:41.235360Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"batch","metadata":{"execution":{"iopub.status.busy":"2024-09-04T12:02:42.568946Z","iopub.execute_input":"2024-09-04T12:02:42.569833Z","iopub.status.idle":"2024-09-04T12:02:42.603915Z","shell.execute_reply.started":"2024-09-04T12:02:42.569788Z","shell.execute_reply":"2024-09-04T12:02:42.602934Z"},"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"{'input_ids': tensor([[32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014,\n         32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014,\n         32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014,\n         32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014,\n         32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014,\n         32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014,\n         32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014,\n         32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014,\n         32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014,\n         32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014,\n         32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014,\n         32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014,\n         32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014,\n         32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014,\n         32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014,\n         32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014,\n         32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014,\n         32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014,\n         32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014,\n         32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014,\n         32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014,\n         32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014,\n         32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014,\n         32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014,\n         32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014,\n         32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014,\n         32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014,\n         32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014,\n         32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014,\n         32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014,\n         32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014,\n         32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014,\n         32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014,\n         32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014,\n         32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014,\n         32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014,\n         32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014,\n         32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32013,\n          7605,   387,   245,  5621,  1719,  1452,   327,   254,  1884,  9840,\n          2040,    25,   380,  6327,   508,  7207,   507,  1171, 12353,  7928,\n          1270,  3314,  1013,    51,  8121,   380,  6327,   508,    29,   323,\n          3341,   938, 10647,     7, 10647,   270,     8,  8474, 13147, 10647,\n           508,  3305,   507,   967,  3341,   938, 10647,     7,    65,    13,\n         24657,     7, 10647,   508,    13,  6787,    57,   650,   270,   477,\n           611,   207,  1171, 12353,  7928,  3314,   323,  3341,   938, 10647,\n             7, 10647,   270,   477,  1171, 12353,  7928,  1171,  6159,  1476,\n            54,  1661,   787,  1195,   309, 19791,  2456,  3314,   323,  3341,\n           938, 10647,     7,  3667,    27,    51,    29,   495,    82,    11,\n           380,  6327,   270,   477,  1171, 12353,  7928,  1171,  6159,  1476,\n            54,  1661,   787,  1195,   309, 19791,  2456,  3314,   323,  3341,\n           938, 10647,     7,  2005,  1208,    11,   380,  6327,   270,   477,\n           207,   611]]), 'attention_mask': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1]])}"},"metadata":{}}]},{"cell_type":"code","source":"tmp=Dataset.from_list(tmp)\ntmp","metadata":{"execution":{"iopub.status.busy":"2024-09-04T12:03:04.368498Z","iopub.execute_input":"2024-09-04T12:03:04.369372Z","iopub.status.idle":"2024-09-04T12:03:05.997740Z","shell.execute_reply.started":"2024-09-04T12:03:04.369329Z","shell.execute_reply":"2024-09-04T12:03:05.996735Z"},"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['instruction', 'output'],\n    num_rows: 78534\n})"},"metadata":{}}]},{"cell_type":"code","source":"# Define the build_instruction_prompt function\ndef build_instruction_prompt(instruction: str):\n    return '''\nYou are an AI programming assistant, utilizing the DeepSeek Coder model, developed by DeepSeek Company, and you only answer questions related to computer science. For politically sensitive questions, security and privacy issues, and other non-computer science questions, you will refuse to answer.\n### Instruction:\n{}\n### Response:\n'''.format(instruction.strip()).lstrip()\n\n# Define the EOT_TOKEN\nEOT_TOKEN = \"<|EOT|>\"","metadata":{"execution":{"iopub.status.busy":"2024-09-04T12:03:09.500761Z","iopub.execute_input":"2024-09-04T12:03:09.501378Z","iopub.status.idle":"2024-09-04T12:03:09.506707Z","shell.execute_reply.started":"2024-09-04T12:03:09.501335Z","shell.execute_reply":"2024-09-04T12:03:09.505775Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"def tokenize_data(data):\n  input_col=tokenizer(data['instruction'],max_length=512,truncation=True,padding=\"max_length\",return_tensors=\"pt\")\n  target_col=tokenizer(data['output'],max_length=512,truncation=True,padding=\"max_length\",return_tensors=\"pt\")\n\n  return {\n      \"input_ids\":input_col[\"input_ids\"],\n      \"attention_mask\":input_col[\"attention_mask\"],\n      \"labels\":target_col[\"input_ids\"]\n  }","metadata":{"execution":{"iopub.status.busy":"2024-09-04T12:03:11.611071Z","iopub.execute_input":"2024-09-04T12:03:11.611460Z","iopub.status.idle":"2024-09-04T12:03:11.617408Z","shell.execute_reply.started":"2024-09-04T12:03:11.611422Z","shell.execute_reply":"2024-09-04T12:03:11.616372Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"tokenizer.padding_side = \"right\"\nprint(\"Tokenizing dataset...\")","metadata":{"execution":{"iopub.status.busy":"2024-09-04T13:24:06.354020Z","iopub.execute_input":"2024-09-04T13:24:06.354976Z","iopub.status.idle":"2024-09-04T13:24:06.360078Z","shell.execute_reply.started":"2024-09-04T13:24:06.354935Z","shell.execute_reply":"2024-09-04T13:24:06.359137Z"},"trusted":true},"execution_count":90,"outputs":[{"name":"stdout","text":"Tokenizing dataset...\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\"Mapping train data...\")\ntrain=train.map(tokenize_data,batched=True)","metadata":{"execution":{"iopub.status.busy":"2024-09-04T12:03:23.605417Z","iopub.execute_input":"2024-09-04T12:03:23.606127Z","iopub.status.idle":"2024-09-04T12:03:25.453805Z","shell.execute_reply.started":"2024-09-04T12:03:23.606069Z","shell.execute_reply":"2024-09-04T12:03:25.452895Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"Mapping train data...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/800 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54fe29881c9549919602139ae354c6c0"}},"metadata":{}}]},{"cell_type":"code","source":"train","metadata":{"execution":{"iopub.status.busy":"2024-09-04T12:03:25.455785Z","iopub.execute_input":"2024-09-04T12:03:25.456262Z","iopub.status.idle":"2024-09-04T12:03:25.462057Z","shell.execute_reply.started":"2024-09-04T12:03:25.456209Z","shell.execute_reply":"2024-09-04T12:03:25.461152Z"},"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],\n    num_rows: 800\n})"},"metadata":{}}]},{"cell_type":"code","source":"print(\"Mappig test data...\")\ntest=test.map(tokenize_data,batched=True)","metadata":{"execution":{"iopub.status.busy":"2024-09-04T12:03:39.245697Z","iopub.execute_input":"2024-09-04T12:03:39.246107Z","iopub.status.idle":"2024-09-04T12:03:39.807467Z","shell.execute_reply.started":"2024-09-04T12:03:39.246067Z","shell.execute_reply":"2024-09-04T12:03:39.806496Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"Mappig test data...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/200 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2515390298cc432faa2e3618a430c8ed"}},"metadata":{}}]},{"cell_type":"code","source":"test","metadata":{"execution":{"iopub.status.busy":"2024-09-04T12:03:40.851594Z","iopub.execute_input":"2024-09-04T12:03:40.852438Z","iopub.status.idle":"2024-09-04T12:03:40.858177Z","shell.execute_reply.started":"2024-09-04T12:03:40.852400Z","shell.execute_reply":"2024-09-04T12:03:40.857304Z"},"trusted":true},"execution_count":38,"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels'],\n    num_rows: 200\n})"},"metadata":{}}]},{"cell_type":"code","source":"train=train.remove_columns([\"instruction\",\"output\"])\ntest=test.remove_columns([\"instruction\",\"output\"])","metadata":{"execution":{"iopub.status.busy":"2024-09-04T12:03:44.906376Z","iopub.execute_input":"2024-09-04T12:03:44.906794Z","iopub.status.idle":"2024-09-04T12:03:44.915475Z","shell.execute_reply.started":"2024-09-04T12:03:44.906753Z","shell.execute_reply":"2024-09-04T12:03:44.914660Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"train","metadata":{"execution":{"iopub.status.busy":"2024-09-04T12:03:45.828194Z","iopub.execute_input":"2024-09-04T12:03:45.828804Z","iopub.status.idle":"2024-09-04T12:03:45.834616Z","shell.execute_reply.started":"2024-09-04T12:03:45.828763Z","shell.execute_reply":"2024-09-04T12:03:45.833650Z"},"trusted":true},"execution_count":40,"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['input_ids', 'attention_mask', 'labels'],\n    num_rows: 800\n})"},"metadata":{}}]},{"cell_type":"code","source":"if torch.cuda.is_available():\n    print(f\"CUDA device: {torch.cuda.get_device_name()}\")\n    device={\"\":0}\n    torch_type=torch.bfloat16\nelse:\n    device=\"cpu\"\n    torch_type=torch.bfloat16\n    print(\"I am begging for mercy already!\")","metadata":{"execution":{"iopub.status.busy":"2024-09-04T12:03:46.749371Z","iopub.execute_input":"2024-09-04T12:03:46.750119Z","iopub.status.idle":"2024-09-04T12:03:46.811231Z","shell.execute_reply.started":"2024-09-04T12:03:46.750074Z","shell.execute_reply":"2024-09-04T12:03:46.810285Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"CUDA device: Tesla P100-PCIE-16GB\n","output_type":"stream"}]},{"cell_type":"code","source":"def print_number_of_trainable_model_parameters(model):\n    trainable_model_params = 0\n    all_model_params = 0\n    for _, param in model.named_parameters():\n        all_model_params += param.numel()\n        if param.requires_grad:\n            trainable_model_params += param.numel()\n    return f'trainable model parameters: {trainable_model_params}\\n \\\n            all model parameters: {all_model_params} \\n \\\n            percentage of trainable model parameters: {(trainable_model_params / all_model_params) * 100} %'","metadata":{"execution":{"iopub.status.busy":"2024-09-04T12:04:02.751031Z","iopub.execute_input":"2024-09-04T12:04:02.751418Z","iopub.status.idle":"2024-09-04T12:04:02.758826Z","shell.execute_reply.started":"2024-09-04T12:04:02.751375Z","shell.execute_reply":"2024-09-04T12:04:02.757525Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"model = AutoModelForCausalLM.from_pretrained(base_model,device_map=device)","metadata":{"execution":{"iopub.status.busy":"2024-09-04T12:19:19.156057Z","iopub.execute_input":"2024-09-04T12:19:19.156444Z","iopub.status.idle":"2024-09-04T12:19:23.460634Z","shell.execute_reply.started":"2024-09-04T12:19:19.156411Z","shell.execute_reply":"2024-09-04T12:19:23.459633Z"},"trusted":true},"execution_count":58,"outputs":[{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--deepseek-ai--deepseek-coder-1.3b-instruct/snapshots/e063262dac8366fc1f28a4da0ff3c50ea66259ca/config.json\nUnrecognized keys in `rope_scaling` for 'rope_type'='linear': {'type'}\nModel config LlamaConfig {\n  \"_name_or_path\": \"deepseek-ai/deepseek-coder-1.3b-instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 32013,\n  \"eos_token_id\": 32021,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 2048,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 5504,\n  \"max_position_embeddings\": 16384,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 16,\n  \"num_hidden_layers\": 24,\n  \"num_key_value_heads\": 16,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-06,\n  \"rope_scaling\": {\n    \"factor\": 4.0,\n    \"rope_type\": \"linear\",\n    \"type\": \"linear\"\n  },\n  \"rope_theta\": 100000,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.44.2\",\n  \"use_cache\": true,\n  \"vocab_size\": 32256\n}\n\nloading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--deepseek-ai--deepseek-coder-1.3b-instruct/snapshots/e063262dac8366fc1f28a4da0ff3c50ea66259ca/model.safetensors\nGenerate config GenerationConfig {\n  \"bos_token_id\": 32013,\n  \"eos_token_id\": 32021\n}\n\nAll model checkpoint weights were used when initializing LlamaForCausalLM.\n\nAll the weights of LlamaForCausalLM were initialized from the model checkpoint at deepseek-ai/deepseek-coder-1.3b-instruct.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\nloading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--deepseek-ai--deepseek-coder-1.3b-instruct/snapshots/e063262dac8366fc1f28a4da0ff3c50ea66259ca/generation_config.json\nGenerate config GenerationConfig {\n  \"bos_token_id\": 32013,\n  \"eos_token_id\": 32021\n}\n\n","output_type":"stream"}]},{"cell_type":"code","source":"print(model)","metadata":{"execution":{"iopub.status.busy":"2024-09-04T12:19:33.489590Z","iopub.execute_input":"2024-09-04T12:19:33.489981Z","iopub.status.idle":"2024-09-04T12:19:33.497292Z","shell.execute_reply.started":"2024-09-04T12:19:33.489940Z","shell.execute_reply":"2024-09-04T12:19:33.496375Z"},"trusted":true},"execution_count":59,"outputs":[{"name":"stdout","text":"LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(32256, 2048)\n    (layers): ModuleList(\n      (0-23): 24 x LlamaDecoderLayer(\n        (self_attn): LlamaSdpaAttention(\n          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n          (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n          (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)\n          (up_proj): Linear(in_features=2048, out_features=5504, bias=False)\n          (down_proj): Linear(in_features=5504, out_features=2048, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-06)\n        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-06)\n      )\n    )\n    (norm): LlamaRMSNorm((2048,), eps=1e-06)\n    (rotary_emb): LlamaRotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=2048, out_features=32256, bias=False)\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"print(print_number_of_trainable_model_parameters(model))","metadata":{"execution":{"iopub.status.busy":"2024-09-04T12:19:42.297205Z","iopub.execute_input":"2024-09-04T12:19:42.297838Z","iopub.status.idle":"2024-09-04T12:19:42.304528Z","shell.execute_reply.started":"2024-09-04T12:19:42.297799Z","shell.execute_reply":"2024-09-04T12:19:42.303629Z"},"trusted":true},"execution_count":60,"outputs":[{"name":"stdout","text":"trainable model parameters: 1346471936\n             all model parameters: 1346471936 \n             percentage of trainable model parameters: 100.0 %\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Nested quantization**\nFor enabling nested quantization, use the bnb_4bit_use_double_quant argument in BitsAndBytesConfig. This will enable a second quantization after the first one to save an additional 0.4 bits per parameter.","metadata":{}},{"cell_type":"code","source":"nf4_config = BitsAndBytesConfig(\n   load_in_4bit=True,\n   bnb_4bit_quant_type=\"nf4\",\n   bnb_4bit_use_double_quant=True,\n   bnb_4bit_compute_dtype=torch.bfloat16\n)\n\nmodel_nf4 = AutoModelForCausalLM.from_pretrained(base_model, quantization_config=nf4_config)","metadata":{"execution":{"iopub.status.busy":"2024-09-04T12:19:52.525182Z","iopub.execute_input":"2024-09-04T12:19:52.525633Z","iopub.status.idle":"2024-09-04T12:19:55.819292Z","shell.execute_reply.started":"2024-09-04T12:19:52.525590Z","shell.execute_reply":"2024-09-04T12:19:55.818490Z"},"trusted":true},"execution_count":61,"outputs":[{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--deepseek-ai--deepseek-coder-1.3b-instruct/snapshots/e063262dac8366fc1f28a4da0ff3c50ea66259ca/config.json\nUnrecognized keys in `rope_scaling` for 'rope_type'='linear': {'type'}\nModel config LlamaConfig {\n  \"_name_or_path\": \"deepseek-ai/deepseek-coder-1.3b-instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 32013,\n  \"eos_token_id\": 32021,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 2048,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 5504,\n  \"max_position_embeddings\": 16384,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 16,\n  \"num_hidden_layers\": 24,\n  \"num_key_value_heads\": 16,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-06,\n  \"rope_scaling\": {\n    \"factor\": 4.0,\n    \"rope_type\": \"linear\",\n    \"type\": \"linear\"\n  },\n  \"rope_theta\": 100000,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.44.2\",\n  \"use_cache\": true,\n  \"vocab_size\": 32256\n}\n\nOverriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own torch_dtype to specify the dtype of the remaining non-linear layers or pass torch_dtype=torch.float16 to remove this warning.\nThe device_map was not initialized. Setting device_map to {'':torch.cuda.current_device()}. If you want to use the model for inference, please set device_map ='auto' \n`low_cpu_mem_usage` was None, now set to True since model is quantized.\nloading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--deepseek-ai--deepseek-coder-1.3b-instruct/snapshots/e063262dac8366fc1f28a4da0ff3c50ea66259ca/model.safetensors\nInstantiating LlamaForCausalLM model under default dtype torch.float16.\nGenerate config GenerationConfig {\n  \"bos_token_id\": 32013,\n  \"eos_token_id\": 32021\n}\n\nAll model checkpoint weights were used when initializing LlamaForCausalLM.\n\nAll the weights of LlamaForCausalLM were initialized from the model checkpoint at deepseek-ai/deepseek-coder-1.3b-instruct.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\nloading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--deepseek-ai--deepseek-coder-1.3b-instruct/snapshots/e063262dac8366fc1f28a4da0ff3c50ea66259ca/generation_config.json\nGenerate config GenerationConfig {\n  \"bos_token_id\": 32013,\n  \"eos_token_id\": 32021\n}\n\n","output_type":"stream"}]},{"cell_type":"code","source":"print(model_nf4)","metadata":{"execution":{"iopub.status.busy":"2024-09-04T12:20:29.107073Z","iopub.execute_input":"2024-09-04T12:20:29.107492Z","iopub.status.idle":"2024-09-04T12:20:29.115312Z","shell.execute_reply.started":"2024-09-04T12:20:29.107452Z","shell.execute_reply":"2024-09-04T12:20:29.114241Z"},"trusted":true},"execution_count":62,"outputs":[{"name":"stdout","text":"LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(32256, 2048)\n    (layers): ModuleList(\n      (0-23): 24 x LlamaDecoderLayer(\n        (self_attn): LlamaSdpaAttention(\n          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n          (k_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n          (v_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear4bit(in_features=2048, out_features=5504, bias=False)\n          (up_proj): Linear4bit(in_features=2048, out_features=5504, bias=False)\n          (down_proj): Linear4bit(in_features=5504, out_features=2048, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-06)\n        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-06)\n      )\n    )\n    (norm): LlamaRMSNorm((2048,), eps=1e-06)\n    (rotary_emb): LlamaRotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=2048, out_features=32256, bias=False)\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"print(print_number_of_trainable_model_parameters(model_nf4))","metadata":{"execution":{"iopub.status.busy":"2024-09-04T12:20:55.681927Z","iopub.execute_input":"2024-09-04T12:20:55.682335Z","iopub.status.idle":"2024-09-04T12:20:55.689179Z","shell.execute_reply.started":"2024-09-04T12:20:55.682295Z","shell.execute_reply":"2024-09-04T12:20:55.688162Z"},"trusted":true},"execution_count":63,"outputs":[{"name":"stdout","text":"trainable model parameters: 132220928\n             all model parameters: 739346432 \n             percentage of trainable model parameters: 17.883487669282484 %\n","output_type":"stream"}]},{"cell_type":"code","source":"lora_config = LoraConfig(\n    r=64,\n    lora_alpha=16,\n    lora_dropout = 0.1,\n    bias='none',\n    task_type=\"CAUSAL_LM\"\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-04T12:04:42.958932Z","iopub.execute_input":"2024-09-04T12:04:42.959795Z","iopub.status.idle":"2024-09-04T12:04:42.964322Z","shell.execute_reply.started":"2024-09-04T12:04:42.959756Z","shell.execute_reply":"2024-09-04T12:04:42.963314Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"# peft_model = get_peft_model(model, lora_config)\npeft_model = get_peft_model(model_nf4, lora_config)","metadata":{"execution":{"iopub.status.busy":"2024-09-04T12:23:02.943693Z","iopub.execute_input":"2024-09-04T12:23:02.944106Z","iopub.status.idle":"2024-09-04T12:23:03.094171Z","shell.execute_reply.started":"2024-09-04T12:23:02.944068Z","shell.execute_reply":"2024-09-04T12:23:03.093257Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"print(peft_model)","metadata":{"execution":{"iopub.status.busy":"2024-09-04T12:23:06.207201Z","iopub.execute_input":"2024-09-04T12:23:06.208105Z","iopub.status.idle":"2024-09-04T12:23:06.218670Z","shell.execute_reply.started":"2024-09-04T12:23:06.208063Z","shell.execute_reply":"2024-09-04T12:23:06.217614Z"},"trusted":true},"execution_count":65,"outputs":[{"name":"stdout","text":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): LlamaForCausalLM(\n      (model): LlamaModel(\n        (embed_tokens): Embedding(32256, 2048)\n        (layers): ModuleList(\n          (0-23): 24 x LlamaDecoderLayer(\n            (self_attn): LlamaSdpaAttention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n              (rotary_emb): LlamaRotaryEmbedding()\n            )\n            (mlp): LlamaMLP(\n              (gate_proj): Linear4bit(in_features=2048, out_features=5504, bias=False)\n              (up_proj): Linear4bit(in_features=2048, out_features=5504, bias=False)\n              (down_proj): Linear4bit(in_features=5504, out_features=2048, bias=False)\n              (act_fn): SiLU()\n            )\n            (input_layernorm): LlamaRMSNorm((2048,), eps=1e-06)\n            (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-06)\n          )\n        )\n        (norm): LlamaRMSNorm((2048,), eps=1e-06)\n        (rotary_emb): LlamaRotaryEmbedding()\n      )\n      (lm_head): Linear(in_features=2048, out_features=32256, bias=False)\n    )\n  )\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"print(print_number_of_trainable_model_parameters(peft_model))","metadata":{"execution":{"iopub.status.busy":"2024-09-04T12:23:12.248127Z","iopub.execute_input":"2024-09-04T12:23:12.249134Z","iopub.status.idle":"2024-09-04T12:23:12.257437Z","shell.execute_reply.started":"2024-09-04T12:23:12.249093Z","shell.execute_reply":"2024-09-04T12:23:12.256415Z"},"trusted":true},"execution_count":66,"outputs":[{"name":"stdout","text":"trainable model parameters: 6291456\n             all model parameters: 745637888 \n             percentage of trainable model parameters: 0.8437682823327776 %\n","output_type":"stream"}]},{"cell_type":"code","source":"print(f\"BF16 support is {transformers.file_utils.is_torch_bf16_available()}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-04T12:05:32.416377Z","iopub.execute_input":"2024-09-04T12:05:32.417123Z","iopub.status.idle":"2024-09-04T12:05:32.423637Z","shell.execute_reply.started":"2024-09-04T12:05:32.417060Z","shell.execute_reply":"2024-09-04T12:05:32.422614Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stdout","text":"BF16 support is True\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/utils/import_utils.py:455: FutureWarning: The util is_torch_bf16_available is deprecated, please use is_torch_bf16_gpu_available or is_torch_bf16_cpu_available instead according to whether it's used with cpu or gpu\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"./results\",\n    run_name =\"./loggings\",\n    overwrite_output_dir=True,\n    eval_strategy=\"epoch\",\n    learning_rate=5e-5, # default, change to 1e-3 on epoch>4\n    gradient_accumulation_steps=1, # if CUDA out of memory = 3,4\n#     per_device_train_batch_size=8,\n#     per_device_eval_batch_size=8,\n    auto_find_batch_size = True, # for CUDA out of memory \n    weight_decay=0.01,\n    num_train_epochs=1, # test=1, min=4, max=10\n    bf16=True,\n    optim=\"adamw_hf\", # torch.optim.AdamW\n    save_strategy=\"no\",\n    log_level=\"info\",\n    logging_first_step=True,\n    report_to='none' ## can be wandb, but we dont need right now!\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-04T13:26:39.505719Z","iopub.execute_input":"2024-09-04T13:26:39.506733Z","iopub.status.idle":"2024-09-04T13:26:39.538682Z","shell.execute_reply.started":"2024-09-04T13:26:39.506688Z","shell.execute_reply":"2024-09-04T13:26:39.537853Z"},"trusted":true},"execution_count":91,"outputs":[{"name":"stderr","text":"PyTorch: setting up devices\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer = SFTTrainer(\n    model=model,\n    train_dataset=train,\n    eval_dataset=test,\n    peft_config=lora_config,\n    dataset_text_field=\"text\",\n    max_seq_length=None,\n    tokenizer=tokenizer,\n    args=training_args,\n    packing=False,\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-04T13:26:55.944401Z","iopub.execute_input":"2024-09-04T13:26:55.945261Z","iopub.status.idle":"2024-09-04T13:26:56.171604Z","shell.execute_reply.started":"2024-09-04T13:26:55.945215Z","shell.execute_reply":"2024-09-04T13:26:56.170646Z"},"trusted":true},"execution_count":93,"outputs":[{"name":"stderr","text":"PyTorch: setting up devices\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:292: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:321: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\nUsing auto half precision backend\n","output_type":"stream"}]},{"cell_type":"code","source":"!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2024-09-04T12:06:08.628603Z","iopub.execute_input":"2024-09-04T12:06:08.629006Z","iopub.status.idle":"2024-09-04T12:06:09.732823Z","shell.execute_reply.started":"2024-09-04T12:06:08.628967Z","shell.execute_reply":"2024-09-04T12:06:09.731636Z"},"trusted":true},"execution_count":53,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Wed Sep  4 12:06:09 2024       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla P100-PCIE-16GB           Off |   00000000:00:04.0 Off |                    0 |\n| N/A   37C    P0             33W /  250W |    1785MiB /  16384MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-09-04T13:27:10.030217Z","iopub.execute_input":"2024-09-04T13:27:10.030628Z","iopub.status.idle":"2024-09-04T13:35:39.839903Z","shell.execute_reply.started":"2024-09-04T13:27:10.030588Z","shell.execute_reply":"2024-09-04T13:35:39.838971Z"},"trusted":true},"execution_count":94,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n***** Running training *****\n  Num examples = 800\n  Num Epochs = 1\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 100\n  Number of trainable parameters = 6,291,456\n***** Running training *****\n  Num examples = 800\n  Num Epochs = 1\n  Instantaneous batch size per device = 8\n  Training with DataParallel so batch size has been adjusted to: 4\n  Total train batch size (w. parallel, distributed & accumulation) = 4\n  Gradient Accumulation steps = 1\n  Total optimization steps = 200\n  Number of trainable parameters = 6,291,456\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [200/200 08:22, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.292400</td>\n      <td>1.043586</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 8\nWe detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n","output_type":"stream"},{"execution_count":94,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=200, training_loss=1.21825543820858, metrics={'train_runtime': 504.8575, 'train_samples_per_second': 1.585, 'train_steps_per_second': 0.396, 'total_flos': 3162201548390400.0, 'train_loss': 1.21825543820858, 'epoch': 1.0})"},"metadata":{}}]},{"cell_type":"code","source":"!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2024-09-04T13:35:47.311402Z","iopub.execute_input":"2024-09-04T13:35:47.312182Z","iopub.status.idle":"2024-09-04T13:35:48.465814Z","shell.execute_reply.started":"2024-09-04T13:35:47.312142Z","shell.execute_reply":"2024-09-04T13:35:48.464761Z"},"trusted":true},"execution_count":95,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Wed Sep  4 13:35:48 2024       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla P100-PCIE-16GB           Off |   00000000:00:04.0 Off |                    0 |\n| N/A   62C    P0             41W /  250W |   15851MiB /  16384MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer.model.save_pretrained(new_model)\ntrainer.tokenizer.save_pretrained(new_model)","metadata":{"execution":{"iopub.status.busy":"2024-09-04T13:35:49.402768Z","iopub.execute_input":"2024-09-04T13:35:49.403213Z","iopub.status.idle":"2024-09-04T13:35:49.786649Z","shell.execute_reply.started":"2024-09-04T13:35:49.403174Z","shell.execute_reply":"2024-09-04T13:35:49.785651Z"},"trusted":true},"execution_count":96,"outputs":[{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--deepseek-ai--deepseek-coder-1.3b-instruct/snapshots/e063262dac8366fc1f28a4da0ff3c50ea66259ca/config.json\nUnrecognized keys in `rope_scaling` for 'rope_type'='linear': {'type'}\nModel config LlamaConfig {\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 32013,\n  \"eos_token_id\": 32021,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 2048,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 5504,\n  \"max_position_embeddings\": 16384,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 16,\n  \"num_hidden_layers\": 24,\n  \"num_key_value_heads\": 16,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-06,\n  \"rope_scaling\": {\n    \"factor\": 4.0,\n    \"rope_type\": \"linear\",\n    \"type\": \"linear\"\n  },\n  \"rope_theta\": 100000,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.44.2\",\n  \"use_cache\": true,\n  \"vocab_size\": 32256\n}\n\ntokenizer config file saved in CODEX-deepseek-coder-1.3b-instruct/tokenizer_config.json\nSpecial tokens file saved in CODEX-deepseek-coder-1.3b-instruct/special_tokens_map.json\n","output_type":"stream"},{"execution_count":96,"output_type":"execute_result","data":{"text/plain":"('CODEX-deepseek-coder-1.3b-instruct/tokenizer_config.json',\n 'CODEX-deepseek-coder-1.3b-instruct/special_tokens_map.json',\n 'CODEX-deepseek-coder-1.3b-instruct/tokenizer.json')"},"metadata":{}}]},{"cell_type":"code","source":"torch.cuda.empty_cache()  # release CUDA memory","metadata":{"execution":{"iopub.status.busy":"2024-09-04T13:36:11.045340Z","iopub.execute_input":"2024-09-04T13:36:11.045771Z","iopub.status.idle":"2024-09-04T13:36:11.355759Z","shell.execute_reply.started":"2024-09-04T13:36:11.045716Z","shell.execute_reply":"2024-09-04T13:36:11.354641Z"},"trusted":true},"execution_count":97,"outputs":[]},{"cell_type":"markdown","source":"apex optimizer","metadata":{}},{"cell_type":"code","source":"!git clone https://github.com/NVIDIA/apex\n%cd apex\n# if pip >= 23.1 (ref: https://pip.pypa.io/en/stable/news/#v23-1) which supports multiple `--config-settings` with the same key... \n!pip install -v --disable-pip-version-check --no-cache-dir --no-build-isolation --config-settings \"--build-option=--cpp_ext\" --config-settings \"--build-option=--cuda_ext\" ./\n# otherwise\n# !pip install -v --disable-pip-version-check --no-cache-dir --no-build-isolation --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./","metadata":{"execution":{"iopub.status.busy":"2024-09-04T13:19:33.107668Z","iopub.execute_input":"2024-09-04T13:19:33.108053Z","iopub.status.idle":"2024-09-04T13:19:33.113027Z","shell.execute_reply.started":"2024-09-04T13:19:33.108019Z","shell.execute_reply":"2024-09-04T13:19:33.111856Z"},"trusted":true},"execution_count":89,"outputs":[]},{"cell_type":"code","source":"# print('Successfully installed apex-0.1')","metadata":{"execution":{"iopub.status.busy":"2024-09-04T13:05:24.237887Z","iopub.execute_input":"2024-09-04T13:05:24.238776Z","iopub.status.idle":"2024-09-04T13:05:24.244299Z","shell.execute_reply.started":"2024-09-04T13:05:24.238730Z","shell.execute_reply":"2024-09-04T13:05:24.243335Z"},"trusted":true},"execution_count":76,"outputs":[{"name":"stdout","text":"Successfully installed apex-0.1\n","output_type":"stream"}]},{"cell_type":"code","source":"import apex.optimizers.FusedAdam","metadata":{"execution":{"iopub.status.busy":"2024-09-04T13:09:02.621950Z","iopub.execute_input":"2024-09-04T13:09:02.622385Z","iopub.status.idle":"2024-09-04T13:09:02.668628Z","shell.execute_reply.started":"2024-09-04T13:09:02.622344Z","shell.execute_reply":"2024-09-04T13:09:02.667289Z"},"trusted":true},"execution_count":80,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[80], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mapex\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mFusedAdam\u001b[39;00m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'apex.optimizers'"],"ename":"ModuleNotFoundError","evalue":"No module named 'apex.optimizers'","output_type":"error"}]},{"cell_type":"code","source":"!pip show apex\n!pip list | grep apex","metadata":{"execution":{"iopub.status.busy":"2024-09-04T13:14:46.671971Z","iopub.execute_input":"2024-09-04T13:14:46.672386Z","iopub.status.idle":"2024-09-04T13:15:02.578796Z","shell.execute_reply.started":"2024-09-04T13:14:46.672348Z","shell.execute_reply":"2024-09-04T13:15:02.577561Z"},"trusted":true},"execution_count":81,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Name: apex\nVersion: 0.1\nSummary: PyTorch Extensions written by NVIDIA\nHome-page: \nAuthor: \nAuthor-email: \nLicense: \nLocation: /opt/conda/lib/python3.10/site-packages\nRequires: packaging\nRequired-by: \n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"apex                                     0.1\n","output_type":"stream"}]},{"cell_type":"code","source":"help(apex)","metadata":{"execution":{"iopub.status.busy":"2024-09-04T13:18:09.915274Z","iopub.execute_input":"2024-09-04T13:18:09.916177Z","iopub.status.idle":"2024-09-04T13:18:09.922562Z","shell.execute_reply.started":"2024-09-04T13:18:09.916137Z","shell.execute_reply":"2024-09-04T13:18:09.921629Z"},"trusted":true},"execution_count":88,"outputs":[{"name":"stdout","text":"Help on package apex:\n\nNAME\n    apex\n\nPACKAGE CONTENTS\n    apex (package)\n    setup\n\nFILE\n    (built-in)\n\n\n","output_type":"stream"}]}]}