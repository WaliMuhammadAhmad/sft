{"cells":[{"cell_type":"code","execution_count":4,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-09-04T11:58:01.199561Z","iopub.status.busy":"2024-09-04T11:58:01.199235Z","iopub.status.idle":"2024-09-04T11:58:35.380301Z","shell.execute_reply":"2024-09-04T11:58:35.378963Z","shell.execute_reply.started":"2024-09-04T11:58:01.199527Z"},"trusted":true},"outputs":[],"source":["# %%capture \n","# %pip install -r requirements.txt"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-09-04T11:58:35.383030Z","iopub.status.busy":"2024-09-04T11:58:35.382624Z","iopub.status.idle":"2024-09-04T11:58:55.214036Z","shell.execute_reply":"2024-09-04T11:58:55.213204Z","shell.execute_reply.started":"2024-09-04T11:58:35.382985Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["d:\\Model\\env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import torch\n","import transformers\n","\n","from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    TrainingArguments,\n",")\n","from peft import LoraConfig\n","from trl import SFTTrainer\n","from datasets import load_dataset\n","from huggingface_hub import login"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-09-04T12:00:24.705464Z","iopub.status.busy":"2024-09-04T12:00:24.704677Z","iopub.status.idle":"2024-09-04T12:00:24.711009Z","shell.execute_reply":"2024-09-04T12:00:24.709892Z","shell.execute_reply.started":"2024-09-04T12:00:24.705422Z"},"trusted":true},"outputs":[],"source":["model = \"deepseek-coder-1.3b-instruct\"  # model to be fine-tuned\n","\n","base_model = f\"deepseek-ai/{model}\"\n","\n","new_model = f\"CODEX-{model}\"  # fine-tunned model\n","\n","dataset = \"CodexAI/Deepseek-Coder\"  # dataset name at huggingface"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-09-04T12:00:28.204265Z","iopub.status.busy":"2024-09-04T12:00:28.203653Z","iopub.status.idle":"2024-09-04T12:00:28.532067Z","shell.execute_reply":"2024-09-04T12:00:28.531161Z","shell.execute_reply.started":"2024-09-04T12:00:28.204225Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Token is valid (permission: write).\n","Your token has been saved in your configured git credential helpers (manager).\n","Your token has been saved to C:\\Users\\walim\\.cache\\huggingface\\token\n","Login successful\n"]}],"source":["login('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',add_to_git_credential=True)"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["df = load_dataset(dataset)"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-09-04T12:01:16.655522Z","iopub.status.busy":"2024-09-04T12:01:16.654696Z","iopub.status.idle":"2024-09-04T12:01:16.664035Z","shell.execute_reply":"2024-09-04T12:01:16.663103Z","shell.execute_reply.started":"2024-09-04T12:01:16.655473Z"},"trusted":true},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['instruction', 'output'],\n","        num_rows: 78434\n","    })\n","    test: Dataset({\n","        features: ['instruction', 'output'],\n","        num_rows: 100\n","    })\n","})"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["df"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-09-04T12:01:31.139317Z","iopub.status.busy":"2024-09-04T12:01:31.138458Z","iopub.status.idle":"2024-09-04T12:01:31.151413Z","shell.execute_reply":"2024-09-04T12:01:31.150518Z","shell.execute_reply.started":"2024-09-04T12:01:31.139274Z"},"trusted":true},"outputs":[{"data":{"text/plain":["{'instruction': 'Generate a unit test case for the following Java method: BundleEntityBuilder { public Map<String, BundleArtifacts> build(Bundle bundle, BundleType bundleType, Document document, ProjectInfo projectInfo) { return build(bundle, bundleType, document, projectInfo, false); } @Inject  BundleEntityBuilder(final Set<EntityBuilder> entityBuilders, final BundleDocumentBuilder bundleDocumentBuilder,\\n                        final BundleMetadataBuilder bundleMetadataBuilder,\\n                        final EntityTypeRegistry entityTypeRegistry, PrivateKeyImportContextBuilder privateKeyImportContextBuilder); Map<String, BundleArtifacts> build(Bundle bundle, BundleType bundleType,\\n                                              Document document, ProjectInfo projectInfo); Map<String, BundleArtifacts> build(Bundle bundle, BundleType bundleType,\\n                                              Document document, ProjectInfo projectInfo, boolean generateMetadata); void addPrivateKeyContexts(Bundle bundle, ProjectInfo projectInfo, BundleArtifacts bundleArtifacts,\\n                                      Document document); @VisibleForTesting Set<EntityBuilder> getEntityBuilders();  }',\n"," 'output': 'The unit test case for the given Java method is: @Test public void testAnnotatedEncassDeleteBundle_WithSharedPolicyFragment() { BundleEntityBuilder builder = createBundleEntityBuilder(); Bundle bundle = createBundleWithPolicyFragment(true, projectInfo); Encass encass = buildTestEncassWithAnnotation(TEST_GUID, TEST_ENCASS_POLICY, false); bundle.putAllEncasses(ImmutableMap.of(TEST_ENCASS, encass)); Map<String, BundleArtifacts> bundles = builder.build(bundle, DEPLOYMENT, DocumentTools.INSTANCE.getDocumentBuilder().newDocument(), projectInfo); assertNotNull(bundles); assertEquals(1, bundles.size()); Element deleteBundleElement = bundles.get(TEST_ENCASS_ANNOTATION_NAME + \"-1.0\").getDeleteBundle().getElement(); assertNotNull(deleteBundleElement); assertEquals(BundleDocumentBuilder.GATEWAY_MANAGEMENT, deleteBundleElement.getAttribute(BundleDocumentBuilder.L7)); assertEquals(BUNDLE, deleteBundleElement.getTagName()); final int expectedElementCountBundle = 2; final Element references = getSingleChildElement(deleteBundleElement, REFERENCES); assertNotNull(references); final List<Element> itemList = getChildElements(references, ITEM); assertNotNull(itemList); assertEquals(expectedElementCountBundle, itemList.size()); final Element item2 = itemList.get(1); assertEquals(\"::\" + projectInfo.getGroupName() + \".\" + TEST_ENCASS_ANNOTATION_NAME + \"::\" + TEST_ENCASS_POLICY + \"::\" + projectInfo.getVersion() , getSingleChildElementTextContent(item2, NAME)); assertEquals(EntityTypes.POLICY_TYPE, getSingleChildElementTextContent(item2, TYPE)); assertNotNull(getSingleChildElement(item2, RESOURCE)); final Element item3 = itemList.get(0); assertEquals(\"::\" + projectInfo.getGroupName() + \".\" + TEST_ENCASS_ANNOTATION_NAME + \"::\" + TEST_ENCASS + \"::\" + projectInfo.getVersion(), getSingleChildElementTextContent(item3, NAME)); assertEquals(EntityTypes.ENCAPSULATED_ASSERTION_TYPE, getSingleChildElementTextContent(item3, TYPE)); assertNotNull(getSingleChildElement(item3, RESOURCE)); final Element mappings = getSingleChildElement(deleteBundleElement, MAPPINGS); assertNotNull(mappings); final List<Element> mappingItemList = getChildElements(mappings, MAPPING); assertEquals(expectedElementCountBundle, mappingItemList.size()); Set<String> propertyValues; final Element mapping2 = mappingItemList.get(1); final Element mapping2Properties = getSingleChildElement(mapping2, PROPERTIES); propertyValues = getChildElementsTextContents(mapping2Properties, PROPERTY); assertTrue(propertyValues.contains(\"::\" + projectInfo.getGroupName() + \".\" + TEST_ENCASS_ANNOTATION_NAME + \"::\" + TEST_ENCASS_POLICY + \"::\" + projectInfo.getVersion())); assertEquals(MappingActions.DELETE, mapping2.getAttribute(\"action\")); assertEquals(EntityTypes.POLICY_TYPE, mapping2.getAttribute(\"type\")); final Element mapping3 = mappingItemList.get(0); final Element mapping3Properties = getSingleChildElement(mapping3, PROPERTIES); propertyValues = getChildElementsTextContents(mapping3Properties, PROPERTY); assertTrue(propertyValues.contains(\"::\" + projectInfo.getGroupName() + \".\" + TEST_ENCASS_ANNOTATION_NAME + \"::\" + TEST_ENCASS + \"::\" + projectInfo.getVersion())); assertEquals(MappingActions.DELETE, mapping3.getAttribute(\"action\")); assertEquals(EntityTypes.ENCAPSULATED_ASSERTION_TYPE, mapping3.getAttribute(\"type\")); }'}"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["df['train'][56]"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["def formatting_func(ex):\n","    ex[\"text\"] = ex[\"instruction\"] + \"\\n\" + ex[\"output\"]\n","    return ex"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Generate a unit test case for the following Java method: BundleEntityBuilder { public Map<String, BundleArtifacts> build(Bundle bundle, BundleType bundleType, Document document, ProjectInfo projectInfo) { return build(bundle, bundleType, document, projectInfo, false); } @Inject  BundleEntityBuilder(final Set<EntityBuilder> entityBuilders, final BundleDocumentBuilder bundleDocumentBuilder,\n","                        final BundleMetadataBuilder bundleMetadataBuilder,\n","                        final EntityTypeRegistry entityTypeRegistry, PrivateKeyImportContextBuilder privateKeyImportContextBuilder); Map<String, BundleArtifacts> build(Bundle bundle, BundleType bundleType,\n","                                              Document document, ProjectInfo projectInfo); Map<String, BundleArtifacts> build(Bundle bundle, BundleType bundleType,\n","                                              Document document, ProjectInfo projectInfo, boolean generateMetadata); void addPrivateKeyContexts(Bundle bundle, ProjectInfo projectInfo, BundleArtifacts bundleArtifacts,\n","                                      Document document); @VisibleForTesting Set<EntityBuilder> getEntityBuilders();  }\n","The unit test case for the given Java method is: @Test public void testAnnotatedEncassDeleteBundle_WithSharedPolicyFragment() { BundleEntityBuilder builder = createBundleEntityBuilder(); Bundle bundle = createBundleWithPolicyFragment(true, projectInfo); Encass encass = buildTestEncassWithAnnotation(TEST_GUID, TEST_ENCASS_POLICY, false); bundle.putAllEncasses(ImmutableMap.of(TEST_ENCASS, encass)); Map<String, BundleArtifacts> bundles = builder.build(bundle, DEPLOYMENT, DocumentTools.INSTANCE.getDocumentBuilder().newDocument(), projectInfo); assertNotNull(bundles); assertEquals(1, bundles.size()); Element deleteBundleElement = bundles.get(TEST_ENCASS_ANNOTATION_NAME + \"-1.0\").getDeleteBundle().getElement(); assertNotNull(deleteBundleElement); assertEquals(BundleDocumentBuilder.GATEWAY_MANAGEMENT, deleteBundleElement.getAttribute(BundleDocumentBuilder.L7)); assertEquals(BUNDLE, deleteBundleElement.getTagName()); final int expectedElementCountBundle = 2; final Element references = getSingleChildElement(deleteBundleElement, REFERENCES); assertNotNull(references); final List<Element> itemList = getChildElements(references, ITEM); assertNotNull(itemList); assertEquals(expectedElementCountBundle, itemList.size()); final Element item2 = itemList.get(1); assertEquals(\"::\" + projectInfo.getGroupName() + \".\" + TEST_ENCASS_ANNOTATION_NAME + \"::\" + TEST_ENCASS_POLICY + \"::\" + projectInfo.getVersion() , getSingleChildElementTextContent(item2, NAME)); assertEquals(EntityTypes.POLICY_TYPE, getSingleChildElementTextContent(item2, TYPE)); assertNotNull(getSingleChildElement(item2, RESOURCE)); final Element item3 = itemList.get(0); assertEquals(\"::\" + projectInfo.getGroupName() + \".\" + TEST_ENCASS_ANNOTATION_NAME + \"::\" + TEST_ENCASS + \"::\" + projectInfo.getVersion(), getSingleChildElementTextContent(item3, NAME)); assertEquals(EntityTypes.ENCAPSULATED_ASSERTION_TYPE, getSingleChildElementTextContent(item3, TYPE)); assertNotNull(getSingleChildElement(item3, RESOURCE)); final Element mappings = getSingleChildElement(deleteBundleElement, MAPPINGS); assertNotNull(mappings); final List<Element> mappingItemList = getChildElements(mappings, MAPPING); assertEquals(expectedElementCountBundle, mappingItemList.size()); Set<String> propertyValues; final Element mapping2 = mappingItemList.get(1); final Element mapping2Properties = getSingleChildElement(mapping2, PROPERTIES); propertyValues = getChildElementsTextContents(mapping2Properties, PROPERTY); assertTrue(propertyValues.contains(\"::\" + projectInfo.getGroupName() + \".\" + TEST_ENCASS_ANNOTATION_NAME + \"::\" + TEST_ENCASS_POLICY + \"::\" + projectInfo.getVersion())); assertEquals(MappingActions.DELETE, mapping2.getAttribute(\"action\")); assertEquals(EntityTypes.POLICY_TYPE, mapping2.getAttribute(\"type\")); final Element mapping3 = mappingItemList.get(0); final Element mapping3Properties = getSingleChildElement(mapping3, PROPERTIES); propertyValues = getChildElementsTextContents(mapping3Properties, PROPERTY); assertTrue(propertyValues.contains(\"::\" + projectInfo.getGroupName() + \".\" + TEST_ENCASS_ANNOTATION_NAME + \"::\" + TEST_ENCASS + \"::\" + projectInfo.getVersion())); assertEquals(MappingActions.DELETE, mapping3.getAttribute(\"action\")); assertEquals(EntityTypes.ENCAPSULATED_ASSERTION_TYPE, mapping3.getAttribute(\"type\")); }\n"]}],"source":["print(formatting_func(df['train'][56])[\"text\"])"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["df = df.map(formatting_func)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Generate a unit test case for the following Java method: CoderUtil { static <T> int[] getNullIndexes(T[] inputs) { int[] nullIndexes = new int[inputs.length]; int idx = 0; for (int i = 0; i < inputs.length; i++) { if (inputs[i] == null) { nullIndexes[idx++] = i; } } return Arrays.copyOf(nullIndexes, idx); } private  CoderUtil();   }\n","The unit test case for the given Java method is: @Test public void testGetNullIndexes() { byte[][] inputs = new byte[numInputs][]; inputs[0] = new byte[chunkSize]; inputs[1] = new byte[chunkSize]; for (int i = 2; i < 7; i++) { inputs[i] = null; } inputs[7] = new byte[chunkSize]; inputs[8] = new byte[chunkSize]; int[] nullIndexes = CoderUtil.getNullIndexes(inputs); assertEquals(2, nullIndexes[0]); assertEquals(3, nullIndexes[1]); assertEquals(4, nullIndexes[2]); assertEquals(5, nullIndexes[3]); assertEquals(6, nullIndexes[4]); }\n"]}],"source":["print(df[\"test\"][\"text\"][0])"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-09-04T12:01:51.227531Z","iopub.status.busy":"2024-09-04T12:01:51.227127Z","iopub.status.idle":"2024-09-04T12:01:51.232422Z","shell.execute_reply":"2024-09-04T12:01:51.231226Z","shell.execute_reply.started":"2024-09-04T12:01:51.227492Z"},"trusted":true},"outputs":[],"source":["train=df['train']\n","test=df['test']"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-09-04T12:01:52.626322Z","iopub.status.busy":"2024-09-04T12:01:52.625913Z","iopub.status.idle":"2024-09-04T12:01:52.632735Z","shell.execute_reply":"2024-09-04T12:01:52.631814Z","shell.execute_reply.started":"2024-09-04T12:01:52.626284Z"},"trusted":true},"outputs":[{"data":{"text/plain":["Dataset({\n","    features: ['instruction', 'output', 'text'],\n","    num_rows: 78434\n","})"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["train"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-09-04T12:02:01.102333Z","iopub.status.busy":"2024-09-04T12:02:01.101961Z","iopub.status.idle":"2024-09-04T12:02:01.108557Z","shell.execute_reply":"2024-09-04T12:02:01.107529Z","shell.execute_reply.started":"2024-09-04T12:02:01.102299Z"},"trusted":true},"outputs":[{"data":{"text/plain":["Dataset({\n","    features: ['instruction', 'output', 'text'],\n","    num_rows: 100\n","})"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["test"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-09-04T12:02:02.537217Z","iopub.status.busy":"2024-09-04T12:02:02.536496Z","iopub.status.idle":"2024-09-04T12:02:04.051296Z","shell.execute_reply":"2024-09-04T12:02:04.050310Z","shell.execute_reply.started":"2024-09-04T12:02:02.537178Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading tokenizer...\n"]}],"source":["print(\"Loading tokenizer...\")\n","tokenizer = AutoTokenizer.from_pretrained(base_model)"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-09-04T12:03:46.750119Z","iopub.status.busy":"2024-09-04T12:03:46.749371Z","iopub.status.idle":"2024-09-04T12:03:46.811231Z","shell.execute_reply":"2024-09-04T12:03:46.810285Z","shell.execute_reply.started":"2024-09-04T12:03:46.750074Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["CUDA device: NVIDIA GeForce RTX 3060 Ti\n"]}],"source":["if torch.cuda.is_available():\n","    print(f\"CUDA device: {torch.cuda.get_device_name()}\")\n","    device={\"\":0}\n","    torch_type=torch.bfloat16\n","else:\n","    device=\"cpu\"\n","    torch_type=torch.bfloat16\n","    print(\"I am begging for mercy already!\")"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["sdpa_kernel = \"flash\"\n","torch.backends.cuda.enable_mem_efficient_sdp(sdpa_kernel == \"mem\")\n","torch.backends.cuda.enable_flash_sdp(sdpa_kernel == \"flash\")\n","torch.backends.cuda.enable_math_sdp(sdpa_kernel == \"math\")"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-09-04T12:19:19.156444Z","iopub.status.busy":"2024-09-04T12:19:19.156057Z","iopub.status.idle":"2024-09-04T12:19:23.460634Z","shell.execute_reply":"2024-09-04T12:19:23.459633Z","shell.execute_reply.started":"2024-09-04T12:19:19.156411Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Unrecognized keys in `rope_scaling` for 'rope_type'='linear': {'type'}\n"]}],"source":["model = AutoModelForCausalLM.from_pretrained(base_model, device_map=device, torch_dtype=torch_type, attn_implementation=\"sdpa\")"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-09-04T12:19:33.489981Z","iopub.status.busy":"2024-09-04T12:19:33.489590Z","iopub.status.idle":"2024-09-04T12:19:33.497292Z","shell.execute_reply":"2024-09-04T12:19:33.496375Z","shell.execute_reply.started":"2024-09-04T12:19:33.489940Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["LlamaForCausalLM(\n","  (model): LlamaModel(\n","    (embed_tokens): Embedding(32256, 2048)\n","    (layers): ModuleList(\n","      (0-23): 24 x LlamaDecoderLayer(\n","        (self_attn): LlamaSdpaAttention(\n","          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n","          (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n","          (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n","          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n","          (rotary_emb): LlamaRotaryEmbedding()\n","        )\n","        (mlp): LlamaMLP(\n","          (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)\n","          (up_proj): Linear(in_features=2048, out_features=5504, bias=False)\n","          (down_proj): Linear(in_features=5504, out_features=2048, bias=False)\n","          (act_fn): SiLU()\n","        )\n","        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-06)\n","        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-06)\n","      )\n","    )\n","    (norm): LlamaRMSNorm((2048,), eps=1e-06)\n","    (rotary_emb): LlamaRotaryEmbedding()\n","  )\n","  (lm_head): Linear(in_features=2048, out_features=32256, bias=False)\n",")\n"]}],"source":["print(model)"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-09-04T12:19:42.297838Z","iopub.status.busy":"2024-09-04T12:19:42.297205Z","iopub.status.idle":"2024-09-04T12:19:42.304528Z","shell.execute_reply":"2024-09-04T12:19:42.303629Z","shell.execute_reply.started":"2024-09-04T12:19:42.297799Z"},"trusted":true},"outputs":[{"data":{"text/plain":["torch.bfloat16"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["model.dtype"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-09-04T12:04:42.959795Z","iopub.status.busy":"2024-09-04T12:04:42.958932Z","iopub.status.idle":"2024-09-04T12:04:42.964322Z","shell.execute_reply":"2024-09-04T12:04:42.963314Z","shell.execute_reply.started":"2024-09-04T12:04:42.959756Z"},"trusted":true},"outputs":[],"source":["lora_config = LoraConfig(\n","    r=16,\n","    lora_alpha=32,\n","    lora_dropout = 0.1,\n","    target_modules=\"all-linear\",\n","    bias='none',\n","    task_type=\"CAUSAL_LM\"\n",")"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"data":{"text/plain":["LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='CAUSAL_LM', inference_mode=False, r=16, target_modules='all-linear', lora_alpha=32, lora_dropout=0.1, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False))"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["lora_config"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-09-04T12:05:32.417123Z","iopub.status.busy":"2024-09-04T12:05:32.416377Z","iopub.status.idle":"2024-09-04T12:05:32.423637Z","shell.execute_reply":"2024-09-04T12:05:32.422614Z","shell.execute_reply.started":"2024-09-04T12:05:32.417060Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["BF16 support is True\n"]},{"name":"stderr","output_type":"stream","text":["d:\\Model\\env\\Lib\\site-packages\\transformers\\utils\\import_utils.py:455: FutureWarning: The util is_torch_bf16_available is deprecated, please use is_torch_bf16_gpu_available or is_torch_bf16_cpu_available instead according to whether it's used with cpu or gpu\n","  warnings.warn(\n"]}],"source":["print(f\"BF16 support is {transformers.utils.import_utils.is_torch_bf16_gpu_available()}\")   # must check"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-09-04T13:26:39.506733Z","iopub.status.busy":"2024-09-04T13:26:39.505719Z","iopub.status.idle":"2024-09-04T13:26:39.538682Z","shell.execute_reply":"2024-09-04T13:26:39.537853Z","shell.execute_reply.started":"2024-09-04T13:26:39.506688Z"},"trusted":true},"outputs":[],"source":["training_args = TrainingArguments(\n","    output_dir=\"./results\",\n","    run_name =\"./loggings\",\n","    overwrite_output_dir=True,\n","    eval_strategy=\"steps\",\n","    eval_steps=0.10,\n","    learning_rate=5e-4,\n","    gradient_accumulation_steps=4, \n","    per_device_train_batch_size=2,\n","    per_device_eval_batch_size=2,\n","    weight_decay=0.01,\n","    num_train_epochs=1, # for now=1, later=4\n","    bf16=True,\n","    optim=\"adamw_torch_fused\",\n","    save_strategy=\"no\",\n","    log_level=\"info\",\n","    logging_first_step=True,\n","    report_to='none', ## can be wandb, but we dont need right now!\n","    logging_steps=5,\n","    tf32=True,\n","    warmup_ratio=0,\n","    lr_scheduler_type=\"cosine\",\n","    # torch_compile=True # Install pytorch nightly using conda first. x2 - x5 speed\n",")"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-09-04T13:26:55.945261Z","iopub.status.busy":"2024-09-04T13:26:55.944401Z","iopub.status.idle":"2024-09-04T13:26:56.171604Z","shell.execute_reply":"2024-09-04T13:26:56.170646Z","shell.execute_reply.started":"2024-09-04T13:26:55.945215Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["d:\\Model\\env\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length, packing. Will not be supported from version '1.0.0'.\n","\n","Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n","  warnings.warn(message, FutureWarning)\n","d:\\Model\\env\\Lib\\site-packages\\trl\\trainer\\sft_trainer.py:195: UserWarning: You passed a `packing` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n","  warnings.warn(\n","d:\\Model\\env\\Lib\\site-packages\\trl\\trainer\\sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n","  warnings.warn(\n","d:\\Model\\env\\Lib\\site-packages\\trl\\trainer\\sft_trainer.py:321: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n","  warnings.warn(\n","d:\\Model\\env\\Lib\\site-packages\\trl\\trainer\\sft_trainer.py:407: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n","  warnings.warn(\n","Using auto half precision backend\n"]}],"source":["trainer = SFTTrainer(\n","    model=model,\n","    train_dataset=train,\n","    eval_dataset=test,\n","    peft_config=lora_config,\n","    dataset_text_field=\"text\",\n","    max_seq_length=1024,\n","    tokenizer=tokenizer,\n","    args=training_args,\n","    packing=True,\n",")"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["base_model.model.model.embed_tokens.weight --> torch.bfloat16 False\n","base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.0.mlp.gate_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.0.mlp.up_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.0.mlp.down_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.0.input_layernorm.weight --> torch.bfloat16 False\n","base_model.model.model.layers.0.post_attention_layernorm.weight --> torch.bfloat16 False\n","base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.1.self_attn.k_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.1.self_attn.o_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.1.mlp.gate_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.1.mlp.up_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.1.mlp.down_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.1.input_layernorm.weight --> torch.bfloat16 False\n","base_model.model.model.layers.1.post_attention_layernorm.weight --> torch.bfloat16 False\n","base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.2.self_attn.k_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.2.self_attn.o_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.2.mlp.gate_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.2.mlp.up_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.2.mlp.down_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.2.input_layernorm.weight --> torch.bfloat16 False\n","base_model.model.model.layers.2.post_attention_layernorm.weight --> torch.bfloat16 False\n","base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.3.self_attn.k_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.3.self_attn.o_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.3.mlp.gate_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.3.mlp.up_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.3.mlp.down_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.3.input_layernorm.weight --> torch.bfloat16 False\n","base_model.model.model.layers.3.post_attention_layernorm.weight --> torch.bfloat16 False\n","base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.4.self_attn.k_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.4.self_attn.o_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.4.mlp.gate_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.4.mlp.up_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.4.mlp.down_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.4.input_layernorm.weight --> torch.bfloat16 False\n","base_model.model.model.layers.4.post_attention_layernorm.weight --> torch.bfloat16 False\n","base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.5.self_attn.k_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.5.self_attn.o_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.5.mlp.gate_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.5.mlp.up_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.5.mlp.down_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.5.input_layernorm.weight --> torch.bfloat16 False\n","base_model.model.model.layers.5.post_attention_layernorm.weight --> torch.bfloat16 False\n","base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.6.self_attn.k_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.6.self_attn.o_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.6.mlp.gate_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.6.mlp.up_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.6.mlp.down_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.6.input_layernorm.weight --> torch.bfloat16 False\n","base_model.model.model.layers.6.post_attention_layernorm.weight --> torch.bfloat16 False\n","base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.7.self_attn.k_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.7.self_attn.o_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.7.mlp.gate_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.7.mlp.up_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.7.mlp.down_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.7.input_layernorm.weight --> torch.bfloat16 False\n","base_model.model.model.layers.7.post_attention_layernorm.weight --> torch.bfloat16 False\n","base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.8.self_attn.k_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.8.self_attn.o_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.8.mlp.gate_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.8.mlp.up_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.8.mlp.down_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.8.input_layernorm.weight --> torch.bfloat16 False\n","base_model.model.model.layers.8.post_attention_layernorm.weight --> torch.bfloat16 False\n","base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.9.self_attn.k_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.9.self_attn.o_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.9.mlp.gate_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.9.mlp.up_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.9.mlp.down_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.9.input_layernorm.weight --> torch.bfloat16 False\n","base_model.model.model.layers.9.post_attention_layernorm.weight --> torch.bfloat16 False\n","base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.10.self_attn.k_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.10.self_attn.o_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.10.mlp.gate_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.10.mlp.up_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.10.mlp.down_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.10.input_layernorm.weight --> torch.bfloat16 False\n","base_model.model.model.layers.10.post_attention_layernorm.weight --> torch.bfloat16 False\n","base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.11.self_attn.k_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.11.self_attn.o_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.11.mlp.gate_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.11.mlp.up_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.11.mlp.down_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.11.input_layernorm.weight --> torch.bfloat16 False\n","base_model.model.model.layers.11.post_attention_layernorm.weight --> torch.bfloat16 False\n","base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.12.self_attn.k_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.12.self_attn.o_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.12.mlp.gate_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.12.mlp.up_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.12.mlp.down_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.12.input_layernorm.weight --> torch.bfloat16 False\n","base_model.model.model.layers.12.post_attention_layernorm.weight --> torch.bfloat16 False\n","base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.13.self_attn.k_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.13.self_attn.o_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.13.mlp.gate_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.13.mlp.up_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.13.mlp.down_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.13.input_layernorm.weight --> torch.bfloat16 False\n","base_model.model.model.layers.13.post_attention_layernorm.weight --> torch.bfloat16 False\n","base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.14.self_attn.k_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.14.self_attn.o_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.14.mlp.gate_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.14.mlp.up_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.14.mlp.down_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.14.input_layernorm.weight --> torch.bfloat16 False\n","base_model.model.model.layers.14.post_attention_layernorm.weight --> torch.bfloat16 False\n","base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.15.self_attn.k_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.15.self_attn.o_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.15.mlp.gate_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.15.mlp.up_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.15.mlp.down_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.15.input_layernorm.weight --> torch.bfloat16 False\n","base_model.model.model.layers.15.post_attention_layernorm.weight --> torch.bfloat16 False\n","base_model.model.model.layers.16.self_attn.q_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.16.self_attn.k_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.16.self_attn.v_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.16.self_attn.o_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.16.mlp.gate_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.16.mlp.up_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.16.mlp.down_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.16.input_layernorm.weight --> torch.bfloat16 False\n","base_model.model.model.layers.16.post_attention_layernorm.weight --> torch.bfloat16 False\n","base_model.model.model.layers.17.self_attn.q_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.17.self_attn.k_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.17.self_attn.v_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.17.self_attn.o_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.17.mlp.gate_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.17.mlp.up_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.17.mlp.down_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.17.input_layernorm.weight --> torch.bfloat16 False\n","base_model.model.model.layers.17.post_attention_layernorm.weight --> torch.bfloat16 False\n","base_model.model.model.layers.18.self_attn.q_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.18.self_attn.k_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.18.self_attn.v_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.18.self_attn.o_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.18.mlp.gate_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.18.mlp.up_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.18.mlp.down_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.18.input_layernorm.weight --> torch.bfloat16 False\n","base_model.model.model.layers.18.post_attention_layernorm.weight --> torch.bfloat16 False\n","base_model.model.model.layers.19.self_attn.q_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.19.self_attn.k_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.19.self_attn.v_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.19.self_attn.o_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.19.mlp.gate_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.19.mlp.up_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.19.mlp.down_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.19.input_layernorm.weight --> torch.bfloat16 False\n","base_model.model.model.layers.19.post_attention_layernorm.weight --> torch.bfloat16 False\n","base_model.model.model.layers.20.self_attn.q_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.20.self_attn.k_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.20.self_attn.v_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.20.self_attn.o_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.20.mlp.gate_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.20.mlp.up_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.20.mlp.down_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.20.input_layernorm.weight --> torch.bfloat16 False\n","base_model.model.model.layers.20.post_attention_layernorm.weight --> torch.bfloat16 False\n","base_model.model.model.layers.21.self_attn.q_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.21.self_attn.k_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.21.self_attn.v_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.21.self_attn.o_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.21.mlp.gate_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.21.mlp.up_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.21.mlp.down_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.21.input_layernorm.weight --> torch.bfloat16 False\n","base_model.model.model.layers.21.post_attention_layernorm.weight --> torch.bfloat16 False\n","base_model.model.model.layers.22.self_attn.q_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.22.self_attn.k_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.22.self_attn.v_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.22.self_attn.o_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.22.mlp.gate_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.22.mlp.gate_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.22.mlp.gate_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.22.mlp.up_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.22.mlp.up_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.22.mlp.up_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.22.mlp.down_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.22.input_layernorm.weight --> torch.bfloat16 False\n","base_model.model.model.layers.22.post_attention_layernorm.weight --> torch.bfloat16 False\n","base_model.model.model.layers.23.self_attn.q_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.23.self_attn.k_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.23.self_attn.v_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.23.self_attn.o_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.23.mlp.gate_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.23.mlp.gate_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.23.mlp.gate_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.23.mlp.up_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.23.mlp.up_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.23.mlp.up_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.23.mlp.down_proj.base_layer.weight --> torch.bfloat16 False\n","base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight --> torch.float32 True\n","base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight --> torch.float32 True\n","base_model.model.model.layers.23.input_layernorm.weight --> torch.bfloat16 False\n","base_model.model.model.layers.23.post_attention_layernorm.weight --> torch.bfloat16 False\n","base_model.model.model.norm.weight --> torch.bfloat16 False\n","base_model.model.lm_head.weight --> torch.bfloat16 False\n"]}],"source":["for n, p in trainer.model.named_parameters():\n","    print(n, \"-->\", p.dtype, p.requires_grad)"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-09-04T12:06:08.629006Z","iopub.status.busy":"2024-09-04T12:06:08.628603Z","iopub.status.idle":"2024-09-04T12:06:09.732823Z","shell.execute_reply":"2024-09-04T12:06:09.731636Z","shell.execute_reply.started":"2024-09-04T12:06:08.628967Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Sat Sep  7 21:52:23 2024       \n","+-----------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 560.94                 Driver Version: 560.94         CUDA Version: 12.6     |\n","|-----------------------------------------+------------------------+----------------------+\n","| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                        |               MIG M. |\n","|=========================================+========================+======================|\n","|   0  NVIDIA GeForce RTX 3060 Ti   WDDM  |   00000000:01:00.0  On |                  N/A |\n","|  0%   44C    P8             18W /  240W |    3599MiB /   8192MiB |      0%      Default |\n","|                                         |                        |                  N/A |\n","+-----------------------------------------+------------------------+----------------------+\n","                                                                                         \n","+-----------------------------------------------------------------------------------------+\n","| Processes:                                                                              |\n","|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n","|        ID   ID                                                               Usage      |\n","|=========================================================================================|\n","|    0   N/A  N/A       112    C+G   ...4.0_x64__cv1g1gvanyjgm\\WhatsApp.exe      N/A      |\n","|    0   N/A  N/A      4188    C+G   C:\\Windows\\explorer.exe                     N/A      |\n","|    0   N/A  N/A      6256    C+G   ...2txyewy\\StartMenuExperienceHost.exe      N/A      |\n","|    0   N/A  N/A      6288    C+G   ...nt.CBS_cw5n1h2txyewy\\SearchHost.exe      N/A      |\n","|    0   N/A  N/A      6500      C   C:\\Program Files\\Python312\\python.exe       N/A      |\n","|    0   N/A  N/A      7304    C+G   ...oogle\\Chrome\\Application\\chrome.exe      N/A      |\n","|    0   N/A  N/A      9864    C+G   ...t.LockApp_cw5n1h2txyewy\\LockApp.exe      N/A      |\n","|    0   N/A  N/A     10892    C+G   ...US\\ArmouryDevice\\asus_framework.exe      N/A      |\n","|    0   N/A  N/A     11036    C+G   C:\\Users\\walim\\Downloads\\AnyDesk.exe        N/A      |\n","|    0   N/A  N/A     11872    C+G   ...GeForce Experience\\NVIDIA Share.exe      N/A      |\n","|    0   N/A  N/A     13588    C+G   ...86)\\Microsoft OneDrive\\OneDrive.exe      N/A      |\n","|    0   N/A  N/A     14300    C+G   ...a\\Local\\slack\\app-4.39.95\\slack.exe      N/A      |\n","|    0   N/A  N/A     14556    C+G   ...5n1h2txyewy\\ShellExperienceHost.exe      N/A      |\n","|    0   N/A  N/A     15236    C+G   ...Programs\\Microsoft VS Code\\Code.exe      N/A      |\n","|    0   N/A  N/A     15692    C+G   ...on\\128.0.2739.67\\msedgewebview2.exe      N/A      |\n","|    0   N/A  N/A     16784    C+G   ...siveControlPanel\\SystemSettings.exe      N/A      |\n","+-----------------------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"data":{"text/plain":["Dataset({\n","    features: ['input_ids', 'labels'],\n","    num_rows: 50041\n","})"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["trainer.train_dataset"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"data":{"text/plain":["Dataset({\n","    features: ['input_ids', 'labels'],\n","    num_rows: 66\n","})"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["trainer.eval_dataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2024-09-04T13:27:10.030628Z","iopub.status.busy":"2024-09-04T13:27:10.030217Z","iopub.status.idle":"2024-09-04T13:35:39.839903Z","shell.execute_reply":"2024-09-04T13:35:39.838971Z","shell.execute_reply.started":"2024-09-04T13:27:10.030588Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["***** Running training *****\n","  Num examples = 50,041\n","  Num Epochs = 1\n","  Instantaneous batch size per device = 2\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 4\n","  Total optimization steps = 6,255\n","  Number of trainable parameters = 14,991,360\n","  0%|          | 0/6255 [00:00<?, ?it/s]d:\\Model\\env\\Lib\\site-packages\\torch\\autograd\\graph.py:818: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cudnn\\MHA.cpp:672.)\n","  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n","  0%|          | 1/6255 [01:23<144:47:38, 83.35s/it]"]},{"name":"stdout","output_type":"stream","text":["{'loss': 1.2906, 'grad_norm': 0.3088058829307556, 'learning_rate': 0.000499920063948841, 'epoch': 0.0}\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 5/6255 [06:34<131:13:08, 75.58s/it]"]},{"name":"stdout","output_type":"stream","text":["{'loss': 1.0269, 'grad_norm': 0.18496465682983398, 'learning_rate': 0.0004996003197442046, 'epoch': 0.0}\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 7/6255 [09:44<145:51:05, 84.04s/it]"]}],"source":["trainer.train()"]},{"cell_type":"code","execution_count":59,"metadata":{"execution":{"iopub.execute_input":"2024-09-04T13:35:49.403213Z","iopub.status.busy":"2024-09-04T13:35:49.402768Z","iopub.status.idle":"2024-09-04T13:35:49.786649Z","shell.execute_reply":"2024-09-04T13:35:49.785651Z","shell.execute_reply.started":"2024-09-04T13:35:49.403174Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["loading configuration file config.json from cache at C:\\Users\\walim\\.cache\\huggingface\\hub\\models--deepseek-ai--deepseek-coder-1.3b-instruct\\snapshots\\e063262dac8366fc1f28a4da0ff3c50ea66259ca\\config.json\n","Unrecognized keys in `rope_scaling` for 'rope_type'='linear': {'type'}\n","Model config LlamaConfig {\n","  \"architectures\": [\n","    \"LlamaForCausalLM\"\n","  ],\n","  \"attention_bias\": false,\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 32013,\n","  \"eos_token_id\": 32021,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 2048,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 5504,\n","  \"max_position_embeddings\": 16384,\n","  \"mlp_bias\": false,\n","  \"model_type\": \"llama\",\n","  \"num_attention_heads\": 16,\n","  \"num_hidden_layers\": 24,\n","  \"num_key_value_heads\": 16,\n","  \"pretraining_tp\": 1,\n","  \"rms_norm_eps\": 1e-06,\n","  \"rope_scaling\": {\n","    \"factor\": 4.0,\n","    \"rope_type\": \"linear\",\n","    \"type\": \"linear\"\n","  },\n","  \"rope_theta\": 100000,\n","  \"tie_word_embeddings\": false,\n","  \"torch_dtype\": \"bfloat16\",\n","  \"transformers_version\": \"4.44.2\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 32256\n","}\n","\n","tokenizer config file saved in CODEX-deepseek-coder-1.3b-instruct\\tokenizer_config.json\n","Special tokens file saved in CODEX-deepseek-coder-1.3b-instruct\\special_tokens_map.json\n"]},{"data":{"text/plain":["('CODEX-deepseek-coder-1.3b-instruct\\\\tokenizer_config.json',\n"," 'CODEX-deepseek-coder-1.3b-instruct\\\\special_tokens_map.json',\n"," 'CODEX-deepseek-coder-1.3b-instruct\\\\tokenizer.json')"]},"execution_count":59,"metadata":{},"output_type":"execute_result"}],"source":["trainer.model.save_pretrained(new_model)\n","trainer.tokenizer.save_pretrained(new_model)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[],"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.5"}},"nbformat":4,"nbformat_minor":4}
